---
title: "Excercise 3"
author:
- Johan Fredrik Agerup
- Arne Rustad
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2: default
subtitle: TMA4300 Computer intensive statistical methods Spring 2021
header-includes:
- \usepackage{subfig}
- \usepackage[ruled,vlined]{algorithm2e}
- \SetKw{KwBy}{by}
- \SetKwProg{Init}{init}{}{}
- \SetKw{length}{length}
- \SetKw{KwMean}{mean}
- \SetKw{KwSum}{sum}
- \SetKw{KwSd}{sd}
- \SetKw{KwCorr}{corr}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\sd}{\textrm{SD}}
\newcommand{\bias}{\textrm{Bias}}


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(boot)
library(latex2exp)
library(coda)
library(dplyr)
library(tidyverse)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

In this problem we are going to analyze the non-Gaussian time-series data in `data3A\$x`. The data contains a subsequence of length $T = 100$. The target is to compare two different estimators. A plot of the data is given in Fig. \@ref(fig:plot_OG).

```{r plot_OG, fig.height=3, fig.width=4, fig.cap="Plot of the non-Gaussian time series."}
source("probAdata.R")
df_OG_data = data.frame(t = seq(1, 100), Value = data3A$x)
ggplot(data = df_OG_data, aes(x = t, y = Value)) + geom_point()
```

We consider an AR(2) model which is specified by the relation:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t,
\end{equation}

where $e_t$ are iid with zero mean and constant variance. Deriving the two estimators for the residuals we want to compare, we are going to use the the least sum of squared residuals (LS) and least sum of absolute residuals (LA). The minimisers are obtained by minimising the following loss functions with respect to $\mathbf{\beta}$:

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2\\
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|
\end{align}

We denote the minimisers by $\mathbf{\beta}_{LS}$ and $\mathbf{\beta}_{LA}$ (calculated by the script ARp.beta.est). We then define the estimated residuals to be:

\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t -\hat{\beta}_1x_{t-1} -\hat{\beta}_2 x_{t-2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. (Results for $\hat{\epsilon}_t$ obtained by LS and LA can be calculated with ARp.resid).

## A.1

### 1.1 Use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators.

In order to do the resampling we have to pick random consecutive pairs from the original data. In this case where $\mathbf{x} = \{x_1, \dots, x_{100} \}$ we pick such that all possible pairs is given by:

\begin{equation}\label{all_possible_consecutive_pairs}
\begin{pmatrix}
x_1 & x_2 & x_3 & \dots & x_{99}\\
x_2 & x_3 & x_4 & \dots & x_{100}
\end{pmatrix}
\end{equation}

```{r}
source("probAhelp.R", local = knitr::knit_global())
minisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)
eps_residuals_LS = ARp.resid(x = data3A$x, beta = minisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minisers_ar2$LA)

#initialise values for x1 and x2 
#by picking a random consecutive subsequence from the data

"Agenda:
- det er ikke bare 50 mulige par, men 99
- eks: x1x2, x2x3, x3x4, ..."
sample_consecutive = function(x){
  #set seed for reproducability
  #set.seed(2021)
  idx = sample(seq(1,length(x)-1), size = 1, replace = FALSE)
  return(x[idx:(idx+1)])
  # x_reshaped = matrix(unlist(t(x)), byrow=T, 2*length(x)-2, 2)
  # print(x_reshaped)
  # x_sample_pair = x_reshaped[idx,]
  # return(x_sample_pair)
}

x1x2_sample1 = sample_consecutive(x = data3A$x)
print(x1x2_sample1)
```
To do a resampling, initialise values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data.
```{r, fig.height=3, fig.width=4}
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=length(eps_residuals_LS), replace=TRUE))
df_samples_LS = data.frame(t = seq(1,100), Value = calc_x_LS)
ggplot(data = df_samples_LS) + geom_point(aes(x = t, y = Value))
```

```{r, fig.height=3, fig.width=4}
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=length(eps_residuals_LA), replace=TRUE))
df_samples_LA = data.frame(t = seq(1,100), Value = calc_x_LA)
ggplot(data = df_samples_LA) + geom_point(aes(x = t, y = Value))
```

### 1.2 Specifically, estimate the variance and bias of the two estimators.

The two parameters of interest are $\mathbf{\beta}$ and the estimators for the minimisers is $\hat{\mathbf{\beta}}_{LS}$ and $\hat{\mathbf{\beta}}_{LA}$ with respect to the loss functions $Q_{LS}$ and $Q_{LA}$. We want to estimate the variance of the estimators. This is given by $\mathrm{Var}(\hat{\mathbf{\beta}}_{LS})$ and $\mathrm{Var}(\hat{\mathbf{\beta}}_{LA})$ where we use the plug-in principle to estimate the variance. The following algorithm for calculating the variance and bias for the estimators in the residual bootstrapping is then given by:

1. Generate $B$ bootstrap samples $x^{1*}, \dots, x^{B*}$.
2. Evaluate the corresponding parameter estimates: $\hat{\beta}^{*}(b) = s(x^{b*}), b = 1,2, \dots, B$.
3. Estimate the variance by:

\begin{equation}
\hat{\mathrm{Var}}_B = \frac{ \sum_{b=1}^B \left( \hat{\beta}^{*}(b) - \hat{\beta}^{*}(\cdot) \right)^2 }{B-1},
\end{equation}
where:
\begin{equation}
\hat{\beta}^{*}(\cdot) \equiv \frac{1}{B}\sum_{b=1}^B \hat{\beta}^{*}(b)
\end{equation}

4. Calculate bias for LS:
\begin{equation}
\mathrm{bias}(\hat{\beta}_{LS}) =  \hat{\beta}_{LS}^{*}(\cdot) - t(\hat{F}),
\end{equation}
where $\hat{\beta}_{LS}^{*}(\cdot)$ represents the bootstrap mean of the beta estimator.

5. Calculate bias for LA:
\begin{equation}
\mathrm{bias}(\hat{\beta}_{LA}) =  \hat{\beta}_{LA}^{*}(\cdot) - t(\hat{F}),
\end{equation}
where $\hat{\beta}_{LA}^{*}(\cdot)$ represents the bootstrap mean of the beta estimator and $t(\hat{F})$ is the calculated betas from the original data.

```{r}
# bootstrap resampling algo
residual_bootstrap_resampling = function(x, p, B){
  samples_LS = c()
  samples_LA = c()
  beta_est_LS = c()
  beta_est_LA = c()
  e_est_LS = c()
  e_est_LA = c()
  #calculating the minimisers for the original data
  beta_vec_OG = ARp.beta.est(x = x, p = p)
  #calculating the recentered residuals
  eps_LS = ARp.resid(x = x, beta = beta_vec_OG$LS)
  eps_LA = ARp.resid(x = x, beta = beta_vec_OG$LA)
  
  #initializing the x101 vectors
  x101_ls = c()
  x101_la = c()
  for(i in 1:B){
    " Done equally for both methods LS, LA
    Algo:
    1. Sample pair x1 and x2 from the given data
    2 Use help function ARp.filter to:
    2.1 Resample residuals w/replacement from original data
    2.2 Generate bootstrap with initialized values x_pair
    3. Use help function ARp.beta.est to calculate the betas for bootstrap sample
    "
    #sample pair from original time series data
    x_pair = sample_consecutive(x = x)
    
    #sampling by initializing the consecutive pair
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vec_OG$LS, 
                         e = sample(eps_LS, size=length(eps_LS), replace=TRUE))
    #calculate minimisers based upon bootstrap/resampling and adding to list of betas
    beta_vec_star = ARp.beta.est(x = x_LS, p = p)
    beta_est_LS = c(beta_est_LS, beta_vec_star$LS)
    # print(sqrt(var(x_LS)))
    samples_LS = c(samples_LS, x_LS)
    
    #calculate the residual of the iteration for task 2.
    # using bootstrap sample to calc residuals with estimated betas
    e_est_LS = c(e_est_LS, ARp.resid(x = x_LS, beta = beta_vec_star$LS))
    
    #creating the x101 sample by sampling from the residuals and using the estimated betas
    e_101_LS = sample(e_est_LS, size = 1, replace = TRUE)
    x101_ls = c(x101_ls, beta_vec_star$LS[2]*x[length(x)] + beta_vec_star$LS[1]*x[length(x)-1] + e_101_LS)
    
    #sampling by initializing the consecutive pair
    x_LA = ARp.filter(x0 = x_pair, 
                         beta = beta_vec_OG$LA, 
                         e = sample(eps_LA, size=length(eps_LA), replace=TRUE))
    samples_LA = c(samples_LA, x_LA)
    #calculate minimisers based upon bootstrap/resampling and adding to list of betas
    beta_vec_star = ARp.beta.est(x = x_LA, p = p)
    beta_est_LA = c(beta_est_LA, beta_vec_star$LA)
    
    #calculate the residual of the iteration for task 2.
    # using bootstrap sample to calc residuals with estimated betas
    e_est_LA = c(e_est_LA, ARp.resid(x = x_LA, beta = beta_vec_star$LA))
    
    #creating the x101 sample by sampling from the residuals and using the estimated betas
    e_101_LA = sample(e_est_LA, size = 1, replace = TRUE)
    x101_la = c(x101_la, beta_vec_star$LA[2]*x[length(x)] + beta_vec_star$LA[1]*x[length(x)-1] + e_101_LA)
    
  }
  #calculate the estimates of the variance of the estimators
  hat_SE_B_LS = sum((beta_est_LS - mean(beta_est_LS))^2)/(B -1)
  hat_SE_B_LA = sum((beta_est_LA - mean(beta_est_LA))^2)/(B -1)
  return(list(LS = samples_LS, LA = samples_LA,
              betas_LS_est = beta_est_LS,
              betas_LA_est = beta_est_LA,
              est_sd_beta_LS = hat_SE_B_LS,
              est_sd_beta_LA = hat_SE_B_LA,
              bias_beta_LS = mean(beta_est_LS) - beta_vec_OG$LS,
              bias_beta_LA = mean(beta_est_LA) - beta_vec_OG$LA,
              resiudals_LS = e_est_LS,
              resiudals_LA = e_est_LA, 
              x101_LS = x101_ls, 
              x101_LA = x101_la))
}

#estimating  the variance and bias of the two estimators
boot1 = residual_bootstrap_resampling(x = data3A$x, p = 2, B = 2000)
print(paste0("Variance of the betas: LS = ", boot1$est_sd_beta_LS[1]," and LA = ", boot1$est_sd_beta_LA[1]))

print("Bias of the two estimators:")
print(boot1$bias_beta_LS[1])
print(boot1$bias_beta_LA[1])
```


The LS estimator is optimal for Gaussian AR(p) processes. 

### Explaination if the LS estimator is also optimal for this problem.

Questions:
1. Could we use a diagnostic to decide wheter the LS estimator is optimal for this problem or not?
2. For the new estimation of the beta estimators, will they be reweighted in some sense since they are calculated from the bootstrap sample that is already based upon the LS estimator, but without the sampled residuals?


### 2. Computation of a $95 \%$ confidence interval for $x_{101}$ based upon both estimators.

To be able to compute the confidence interval for $x_{101}$ we have to use the results from 1. To simulate the value of $x_{101}$ we need to estimate the residual distribution corresponding to the bootstrapped time series and parameter estimates. The strategy we will follow for this problem is:

1. Compute residual distribution for LS and LA by sampling from the time series data.
2. Sample from the residual distribution to simulate a value $x_{101}$ by using the AR(2) scheme using last 2 values of original data $x_{99}$ and $x_{100}$.
3. The variability of the value $x_{101}$ should reflect our lack of knowledge of the parameter values and about the residuals, so the variance should be larger for the distribution of $x_{101}$'s than for the parameter values and residuals. (?)
4. Find the limits in the prediction interval for $x_{101}$ as quantiles in the simulated $x_{101}$ values by using the stats::quantile()-function using probs = c(0.025, 0.975).

```{r hist_sampled_res_LS, fig.cap="Histograms of the sampled residuals for LS."}
df_PA2 = data.frame(res_LS = boot1$resiudals_LS, res_LA = boot1$resiudals_LA)
ggplot(data = df_PA2) + geom_histogram(aes(x = res_LS))
```
```{r hist_sampled_res_LA, fig.cap="Histograms of the sampled residuals for LA."}
ggplot(data = df_PA2) + geom_histogram(aes(x = res_LA))
```

The histograms of the sampled residuals are plotted in Fig. \@ref(fig:hist_sampled_res_LS) and \@ref(fig:hist_sampled_res_LA).

```{r plot_x101_LS, fig.cap="Plot of the $x_{101}$'s simulated by LS."}
#computing prediction interval for LS
q_x101_LS = stats::quantile(x = boot1$x101_LS, probs = c(0.025, 0.975))

#computing prediction interval for LS
q_x101_LA = stats::quantile(x = boot1$x101_LA, probs = c(0.025, 0.975))

df_x101 = data.frame(ls = boot1$x101_LS, la = boot1$x101_LA, q_LS = q_x101_LS, q_LA = q_x101_LA)

print(df_x101$q_LS[1:2])
print(paste0("The 95% prediction interval for LS is ", q_x101_LS[1], " to ", q_x101_LS[2]))

ggplot(data = df_x101) + geom_histogram(aes(x = ls)) + 
  geom_vline(xintercept = df_x101$q_LS[1], show.legend = TRUE) + 
  geom_vline(xintercept = df_x101$q_LS[2], show.legend = TRUE ) +
  guides(color = q_x101_LS)
```


```{r plot_x101_LA, fig.cap="Plot of the $x_{101}$'s simulated by LA"}
print(paste0("The 95% prediction interval for LA is ", q_x101_LA[1], " to ", q_x101_LA[2]))
ggplot(data = df_x101) + geom_histogram(aes(x = la)) + 
  geom_vline(xintercept = df_x101$q_LA[1], show.legend = TRUE ) + 
  geom_vline(xintercept = df_x101$q_LA[2], show.legend = TRUE )
```

After calculating the $95\%$ prediction interval of the values of $x_{101}$ we can compare with the empricial distribution function defined:
\begin{equation}
\hat{F} = \sum_{i=1}^{n}\frac{1}{n}1(x_i \leq x)
\end{equation}

where $1(\cdot)$ is the indicator function. This is implemented in the code below:

```{r}
emp_CDF = function(x){
  n = length(x)
  x_sorted = sort(x)
  F_hat = seq(0,n, length.out = n)/n
  return(list(x = x_sorted, cdf = F_hat))
}

f1 = emp_CDF(data3A$x)
plot(f1$x, f1$cdf)

```


# Problem B Permutation Test

In this task we look at data that contain measurements of the concentration of bilirubin (mg/dL) in blood samples taken from three young men. Bilirubin is a breakdown product of haemoglobin, which is a principal component of red blood cells. If the liver has suffered degeneration, if the decomposition of haemoglobin is elevated, or if the gall bladder has been  destroyed, large amounts of bilirubin can accumulate in the blood, leading to jaundice. We will use the F-statistic to perform a Permutation test.

?????????????????? Must we say what the F-statistic is defined as?

```{r}
# Fetching the data
df.bilirubin <- read.table("bilirubin.txt", header=T)
```


## 1)

To begin we fit a boxplot of the logarithms of concentrations for each individual.


```{r, fig.width = 6, fig.cap="\\label{fig:boxplot} Boxplot for the logarithms of concentrations for each individual. The unit for the concentrations is $\\textrm{mg/dL}$."}
# Plotting a boxplot for each person
ggplot(df.bilirubin, aes(x = pers, y = log(meas))) + 
  geom_boxplot() + 
  ggtitle("Boxplot of logarithm of concentration") + 
  ylab("log(measurement)") + 
  xlab("Person")
```

From Figure \ref{fig:boxplot} we observe that the measurements for person 3 differ quite drastically from the two other persons, while the measurements for person 1 and person 2 have approximately equal median. The measurements for person 1 does, however, appear to vary more than the measurements for person 2.

Next we fit the linear regression model

\begin{equation} \label{eq:model_B1}
\ln(Y_{i,j}) = \beta_i + \epsilon_{i,j}, \qquad \textrm{with } i = 1,2,3 \textrm{ and } j = 1, \dots, n_i
\end{equation}

where $n_1 = 11$, $n_2 = 10$ and $n_3 = 8$ and $\epsilon_{i,j} \sim \mathcal{N}(0, \sigma^2)$. 

```{r}
model.bilirubin <- lm(log(meas)~pers, data=df.bilirubin)
summary.model.bilurbin = summary.lm(model.bilirubin)
```

??????????????????????? Must we comment on or display the estimated beta coefficents?

Next we use the F-test to test the hypothesis $\beta_1 = \beta_2 = \beta_3$.

\begin{align*}
&\textrm{H$_0$: } \beta_1 = \beta_2 = \beta_3 \\
&\textrm{H$_1$: At least one $\beta_i$ is different from the other $\beta_i$s}
\end{align*}

We use significance level $\alpha = 0.05$ for the hypothesis test.

```{r}
Fval = summary.model.bilurbin$fstatistic
p.value = pf(Fval[names(Fval) == "value"], Fval[names(Fval) == "numdf"], Fval[names(Fval) == "dendf"],
             lower.tail = FALSE)
```

The F-statistic is $`r round(Fval[names(Fval) == "value"], 3)`$ with $(`r Fval[names(Fval) == "numdf"]`, `r Fval[names(Fval) == "dendf"]`)$ degrees of freedom. The corresponding p-value is $`r round(p.value,4)`$. As this p-value is lower than our chosen significance level $\alpha = 0.05$ we reject the null hypothesis and conclude that at least one $\beta_i$ is different from the other.

## 2)

Next we write a function \textbf{permTest} which generates a permutation of the data between the three individuals, consequently fits the model given in (\ref{eq:model_B1}) and finally returns the value of the F-statistic for testing $\beta_1 = \beta_2 = \beta_3$.

```{r}
# Function that performs one iteration of the Permutation test
permTest = function(df) {
  # permute the data
  n = nrow(df)
  obs.shuffled = sample(1:n, n, replace = FALSE)
  df.shuffled = data.frame(meas = df$meas[obs.shuffled], pers =df$pers)
  # fit the model on the shuffled data
  model.shuffled <- lm(log(meas)~pers, data=df.shuffled)
  # calculate the F-statistic and return it
  fstatistic = summary.lm(model.shuffled)$fstatistic[1]
  return(fstatistic)
}
```

## 3)

In this task we perform a permutation test using the function \textbf{permTest} to generate a sample of size 999 for the F-statistic.

```{r, fig.width = 6}
# Function for performing permutation test
permutation.test = function(df, Fval, B = 999, plot.hist = TRUE) {
  "
  Input:
  - df: data frame for which the permutation test should be performed
  - Fval: F-statistic calculated for the model fitted to the original dataset
  - B: number of generated bootstrap samples
  - plot.hist: Boolean determining whether a histogram of the estimated density function
  of the F-statistic under the null hypothesis should be plotted
  Output:
  - A list containing
    -- p.value: The estimated p-value for the permuation test
    -- fstatistic.vec
  "
  # Initialize vector for storing F-statistics
  fstatistic.vec = rep(NA, B)
  # Calculcate F-statistics from permutated datasets
  for (i in 1:B) fstatistic.vec[i] = permTest(df)
  # if hist == TRUE plot a histogram for the estimated density function of the F-statistic
  if (plot.hist) {
    fig.hist = ggplot(data.frame(fstatistic = fstatistic.vec), aes(x = fstatistic, y = ..density..)) +
      
      geom_histogram(bins = 50) + xlab("F-statistic") + ylab("Density") +
      
      ggtitle("Histogram of the F-statistic under the null hypothesis") +
      
      geom_vline(aes(xintercept=Fval, col = "F-statistic calculated from original dataset"), data = data.frame(Fval = Fval[1]), linetype = "dashed") +
      
      theme(legend.position = "bottom") + guides(col = guide_legend("Vertical line"))
      
    print(fig.hist)
  }
  # Calculate the p-value from the permutation test using the histogram as
  # an approximation for the distribution of the F-statistic
  p.value = sum(fstatistic.vec >= Fval) / B
  # Return the p-value and the simulated F-statistics under the null hypothesis
  return(list(p.value = p.value, fstatistic.vec = fstatistic.vec))
}
```


```{r, fig.width = 6, fig.cap="\\label{fig:histogram} Histogram of estimated F-statistics during the Permutation test. The histogram approximates the density function of the F-statistic under the null hypothesis. The vertical, red, dashed line is the original F-statistic value. "}
set.seed(1)
p.value.permutation.test = permutation.test(df.bilirubin, Fval, B = 999)$p.value
```

In Figure \ref{fig:histogram} we plot a histogram of the F-statistics computed during the Permutation test, which is an approximation for the density function of the F-statistic under the null hypothesis. From the plot it appears that the F-statistic computed from the original data is a part of the upper tail of the distribution function for the F-statistic under the null hypothesis.

The permutation test gives a p-value of $`r round(p.value.permutation.test,4)`$ for the hypothesis test. We observe that this value is a bit larger than the p-value we computed in Task C1 which was $`r round(p.value,4)`$. The p-value from the permutation test is larger than our chosen significance level $\alpha = 0.05$ and hence we do not reject the null hypothesis.



# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i), \; i = 1, \dots, n$, we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f(\vect x | \lambda_0) f(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f(y_i | \lambda_1) \right ]
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{equation*}
\begin{split}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \ln \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{split}
\end{equation*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $(\lambda_o^{(t)}, \lambda_1^{(t)})$.

\begin{equation} \label{eq:em_formula_incomplete}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E [x_i | z_i, u_i, \lambda_0^{(t)}]  - \lambda_1 \sum_{i=1}^n \E [y_i | z_i, u_i, \lambda_1^{(t)}
\end{split}
\end{equation}

For the case $u_i = 1$ then we have

\begin{align*}
\E [x_i | z_i, u_i = 1, \lambda_0^{(t)}] &= u_i
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 1, \lambda_1^{(t)}
&= \int_0^{z_i} y_i P \left( y_i | y_i \leq z_i, \lambda_1^{(t)}\right) dy_i \\
&=\int_0^{z_i} y_i \frac{P \left( y_i \cap y_i \leq z_i | \lambda_1^{(t)} \right) }
{P \left( y_i \leq z_i | \lambda_1^{(t)} \right) } dy_i \\
&= \int_0^{z_i} y_i \frac{f(y_i | \lambda_1^{(t)})}{ \int_0^{z_i} f(y_i^* | \lambda_1^{(t)}) dy_i^*} dy_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i^* } dy_i^*}
\int_0^{z_i} y_i \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i} dy_i) \\
&= \frac{1}{1 - e^{- \lambda_1^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_1^{(t)} z_i} - \frac{1}{\lambda_1^{(t)}} e^{- \lambda_1^{(t)} z_i} + \frac{1}{\lambda_1^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_1^{(t)} z_i}}{1 - e^{- \lambda_1^{(t)} z_i}} + \frac{1}{\lambda_1^{(t)}} \\
&= \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
\end{align*}

Correspondlingly, we have for the $u_i = 0$ that

\begin{align*}
\E [x_i | z_i, u_i = 0, \lambda_0^{(t)}
&= \int_0^{z_i} x_i P \left( x_i | x_i \leq z_i, \lambda_0^{(t)}\right) dx_i \\
&=\int_0^{z_i} x_i \frac{P \left( x_i \cap x_i \leq z_i | \lambda_0^{(t)} \right) }
{P \left( x_i \leq z_i | \lambda_0^{(t)} \right) } dx_i \\
&= \int_0^{z_i} x_i \frac{f(x_i | \lambda_0^{(t)})}{ \int_0^{z_i} f(x_i^* | \lambda_0^{(t)}) dx_i^*} dx_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i^* } dx_i^*}
\int_0^{z_i} x_i \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i} dx_i) \\
&= \frac{1}{1 - e^{- \lambda_0^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_0^{(t)} z_i} - \frac{1}{\lambda_0^{(t)}} e^{- \lambda_0^{(t)} z_i} + \frac{1}{\lambda_0^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_0^{(t)} z_i}}{1 - e^{- \lambda_0^{(t)} z_i}} + \frac{1}{\lambda_0^{(t)}} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 0, \lambda_1^{(t)}] &= u_i \\
\end{align*}

Inserting this result into Equation (\ref{eq:em_formula_incomplete}) we get

\begin{equation} \label{eq:em_formula}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
=& \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
\big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
=& n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) \\
&- \lambda_0 \sum_{i=1}^n 
\left [ u_i z_i 
+ (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1} \right)
\right] \\
&- \lambda_1 \sum_{i=1}^n \left [
(1 - u_i) z_i + 
u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1} \right)
\right ]
\end{split}
\end{equation}

## 2)

Let $Q(\lambda_0, \lambda_1)$ be defined

\begin{equation*}
Q(\lambda_0, \lambda_1)
= Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)})
= \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
\end{equation*}

We want to use the Expectation-Maximization (EM) algorithm as defined in Algorithm \ref{em_algorithm} to find the maximum likelihood estimates of $(\lambda_0, \lambda_1)$ iteratively. We denote these estimates $(\hat \lambda_0, \hat \lambda_1)$.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Observed data $(\vect z, \vect u)$ and starting values $\lambda_0^{(0)}, \lambda_1^{(0)} $}
 \KwOut {Maximum likelihood estimates for $(\lambda_0, \lambda_1)$.}
 $t \gets 0$\;
 \While{not converged}{
  $Q(\lambda_0, \lambda_1) = \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]$ \;
  $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)}) = \argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1) $\;
    $t \gets t + 1$ \;
 }
 \Return{$(\hat \lambda_0, \hat \lambda_1) = (\lambda_0^{(t)}, \lambda_1^{(t)})$}
 \caption{EM algorithm to maximize $l(\lambda_0, \lambda_1 |  \vect z, \vect u)$}
 \label{em_algorithm}
\end{algorithm}

In Task C1 we found an expression for $Q(\lambda_0, \lambda_1)$. To be able to implement Algorithm \ref{em_algorithm} we need to find an expression for $\argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1)$. To maximize $Q(\lambda_0, \lambda_1)$ with respect to $(\lambda_0, \lambda_1)$ we set the partial derivatives equal to zero and solve for each of the parameters.

\begin{align}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_0} - \sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ] = 0 \nonumber \\
  \lambda_0 = \frac{n}
  {\sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda0}
\end{align}

\begin{align}
  \frac{\partial}{\partial \lambda_1} Q(\lambda_1, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_1} - \sum_{i=1}^n \left[ (1 - u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ] = 0
  \nonumber \\
  \lambda_1 = \frac{n}
  {\sum_{i=1}^n \left[ (1-u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda1}
\end{align}

To ensure we have found a maximum we also check the second derivatives

\begin{align*}
\frac{\partial^2}{\partial \lambda_0^2} &= \frac{-n}{\lambda_0^2} < 0\\
  \frac{\partial^2}{\partial \lambda_1^2} &= \frac{-n}{\lambda_1^2} < 0
\end{align*}

As the second derivatives are stricly negative for all $\lambda_0$ and $\lambda_1$ we know that we have found the values that maximizes $Q(\lambda_0, \lambda_1)$. From Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) we have a recursive expression for the maximum likelihood estimates of $(\lambda_0, \lambda_1)$.

Below we implement the EM-algorithm for this specific case and find the maximum likelihood estimates when the data is as specified in the files \textbf{z.txt} and \textbf{u.txt} available from the course home page. As stopping criteria we use that the maximum absolute value parameter change from one iteration to the next must be smaller than some chosen $\epsilon$, that is 
$\max \left \{ |\lambda_0^{(t+1)} - \lambda_0^{(t)}|, |\lambda_1^{(t+1)} - \lambda_1^{(t)}| \right \} < \epsilon$.

```{r}
# implementation of the EM algorithm. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
em.algorithm = function(z, u, lambda0, lambda1, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # create vectors for storing lambda0 and lambda1 values for each iteration
  lambda0.vec = c(lambda0)
  lambda1.vec = c(lambda1)
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  t = 1
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0.vec = c(lambda0.vec, 
                       n / sum(u * z + (1 - u) * (1 / lambda0.vec[t] - z / (exp(lambda0.vec[t] * z) - 1) ) )
    )
    lambda1.vec = c(lambda1.vec,
                    n / sum((1 - u) * z + u * (1 / lambda1.vec[t] - z / (exp(lambda1.vec[t] * z) - 1) ) )
    )
    t = t + 1
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0.vec[t] - lambda0.vec[t-1]), abs(lambda1.vec[t] - lambda1.vec[t-1]))
  }
  # get the number of iterations
  n.iter = t-1
  # create a data frame for storing parameter values for each iteration for easy plotting
  df = data.frame(iter = seq(0,n.iter), lambda0 = lambda0.vec, lambda1 = lambda1.vec)
  # same data frame on long format (also for easy plotting)
  df.long = pivot_longer(df, -iter, names_to = "Parameter")
  # return a list with useful values
  return ( list(lambda0 = lambda0.vec[t],lambda1 = lambda1.vec[t],
                lambda0.vec = lambda0.vec, lambda1.vec = lambda1.vec, n.iter = n.iter,
                df = df, df.long = df.long))
}
```


```{r}
# fetch the data
z = read.csv("z.txt")[,1]
u = read.csv("u.txt")[,1]
# find maximum likelihood estimates
result.em = em.algorithm(z, u, 1, 1)
```

The maximum likelihood estimates of $(\lambda_0, \lambda_1)$ are $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 3)`, `r round(result.em$lambda1, 3)`)$. Next we test the convergence of the algorithm for different initial values.

```{r, fig.width=8, fig.height = 3, fig.cap = "\\label{fig:convergence_plot} Visualization of convergence for each of the parameters $(\\lambda_0,\\lambda_1)$ for three different initial values $(\\lambda_0^{(0)}, \\lambda_1^{(0)}) \\in [(1,1), (7,7), (15, 15)]$. As stopping criteria we use $\\textrm{max} [ |\\lambda_0^{(t+1)} - \\lambda_0^{(t)}|, |\\lambda_1^{(t+1)} - \\lambda_1^{(t)}| ] < 10^{-6}$."}
# test for different initial values
# choose the initial value combinations
initial.lambda0 = c(1, 7, 15)
initial.lambda1 = c(1, 7, 15)

# create a data frame to store all the realizations
df.em.long = data.frame()
for (i in 1:length(initial.lambda0)) {
  result.em.i = em.algorithm(z, u, initial.lambda0[i], initial.lambda1[i])
  df.em.long = rbind(df.em.long, cbind(result.em.i$df.long, "initial.value" = toString(i)))
}
# visialize convergence for different initial values
ggplot(df.em.long, aes(x = iter, y = value, col = initial.value)) + geom_line() +
  
  facet_wrap(~Parameter) + guides(col = guide_legend("Initial values")) +
  
  scale_color_discrete(labels = c("(1,1)", "(7,7)", "(15,15)")) +
  
  ggtitle("Convergence plot of estimates")
```

From Figure \ref{fig:convergence_plot} we see that the algorithm quickly converges for all three initial conditions. The estimate for $\lambda_0$ appear to be almost converged after only a single iteration, while the estimate for $\lambda_1$ seem to need around 5 iterations for the same level of precicion. After 16 iterations all three initial positions were converged according to $\epsilon = 10^{-6}$.

## 3)

In this task we use bootstrapping to estimate the standard deviations and the biases of each of $\hat \lambda_0$ and $\hat \lambda_1$ along with an estimate for $\Corr [\hat \lambda_0, \hat \lambda_1]$. First we define som notation. We will bootstrap $B$ new datasets $(\vect z^{b*}, \vect u^{b*}), \; b = 1, \dots, B$. These datasets will each be used to calculate the two estimators of interest $(\hat \lambda_0^{b*}, \hat \lambda_1^{b*})$ using Algorithm \ref{em_algorithm} on $(\vect z^{b*}, \vect u^{b*})$. From the $B$ pairs of parameter estimates we can estimate the standard deviations of $\hat \lambda_0$ and $\hat \lambda_1$ in addition to the correlation between them.

\begin{align*}
  \widehat {\sd}_{B}[\hat \lambda_0] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)^2} \\
  \widehat {\sd}_{B}[\hat \lambda_1] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_1^{b*} -\bar \lambda_1^*)^2} \\
  \widehat {\Corr}_{B}[\hat \lambda_0, \hat \lambda_1] &= \sqrt{\frac{1}{B-1} 
  \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)(\lambda_1^{b*} -\bar \lambda_1^*)}
\end{align*}

where

\begin{align*}
  \bar \lambda_0^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_0^{b*} \\
  \bar \lambda_1^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_1^{b*}.
\end{align*}

Additionally, we can calculate an estimate for the biases of $\hat \lambda_0$ and $\hat \lambda_1$ using the expressions below.

\begin{equation} \label{eq:bias}
  \begin{split}
  \widehat {\bias}_B [\hat \lambda_0] &= \hat \lambda_0 - \bar \lambda_0^{*} \\
  \widehat {\bias}_B [\hat \lambda_1] &= \hat \lambda_1 - \bar \lambda_1^{*}
  \end{split}
\end{equation}

An improved bias estimate can also be calculated, but requires more complex notation to properly explain. Let $F$ be the cumulative distribution function that we assume $(\vect z, \vect u)$ is sampled from and let $\theta = (\lambda_0, \lambda_1) = T(F)$ be a functional of $F$. We let $\hat F$ be the empirical distribution function of the observed data $(\vect z, \vect u)$ and $\hat F^{b*}, \; b = 1, \dots, B$ be the empirical distribution functon of the corresponding bootstrap sample $(\vect z^{b*}, \vect u^{b*})$. The statistical function of bias can be written as $R((\vect Z, \vect U), F) = T(\hat F) - T(F)$. The bias estimate in Equation (\ref{eq:bias}) was derived as $\frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat F) \right)$. According to \cite{} an improved bias estimate is

\begin{equation*}
 \widehat {\bias}_{B, \textrm{improved}}[\hat \lambda_0, \hat \lambda_1] = \hat {R}((\vect Z, \vect U), F) = \frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat \bar F^*) \right)
 = \bar \theta^* - T(\bar F^*)
\end{equation*}

??????????????????????? Should I use widehat over whole R expression?

where

\begin{equation*}
  \bar F^* = \frac{1}{B} \sum_{b=1}^B \hat F^{b*}
\end{equation*}

and $\bar \theta^* = (\bar \lambda_0^*, \bar \lambda_1^*)$.

Let $T_{\lambda_0}(F) = \lambda_0$ and $T_{\lambda_1}(F) = \lambda_1$. Then $T(\bar F^*)$ can be calculated through the EM algorithm with each step being

\begin{align*}
\lambda_0 &= \frac{n} {\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ u_i^{b*} z_i^{b*} 
+ (1 - u_i^{b*}) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_0^{(t)} z_i^{b*} \} - 1} \right) \right]} \\
\lambda_1 &= \frac{n}{\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ (1-u_i^{b*}) z_i^{b*}
+ u_i^{b*} \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_1^{(t)} z_i^{b*} \} - 1} \right) \right ]}
\end{align*}

The derivation for this is not included, but is quite similar to how Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) were derived.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$, observed data $(\vect z, \vect u)$ and the number of bootstrap samples $B$.}
 \KwOut {Estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Calculates two different bias estimates.}
 \Init{}{
 $ \Lambda_B \gets$ Matrix(nrow : $B$, ncol : $2$) \;
 $n \gets \length(\vect z)$ \;
 $\vect z_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 $\vect u_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 }
 \For{$b \gets 1$ \KwTo $B$ \KwBy $1$} {
 $\vect z^{b*} \gets$ bootstrapped version of $\vect z$ of length $n$. \;
 $\vect u^{b*} \gets$ bootstrapped version of $\vect u$ of length $n$. \;
 $(\lambda_0^{b*}, \lambda_1^{b*}) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z^{b*}, \vect u^{b*})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\Lambda_B[b,] \gets (\lambda_0^{b*}, \lambda_1^{b*})$  \;
 $\vect z_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect z^{b*}$ \;
 $\vect u_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect u^{b*}$ \;
 }
 $\widehat {\sd}_B[\hat \lambda_0] \gets \KwSd(\Lambda[,1])$\;
 $\widehat {\sd}_B[\hat \lambda_1] \gets \KwSd(\Lambda[,2])$\;
 $\widehat {\Corr}[\hat \lambda_0, \hat \lambda_1] \gets \KwCorr(\Lambda[,1], \Lambda[,2])$\;
 $\bar \lambda_0^* \gets \KwMean(\Lambda[,1])$\;
 $\bar \lambda_1^* \gets \KwMean(\Lambda[,2])$\;
 $(\hat \lambda_0, \hat \lambda_1) \gets$  maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z, \vect u)$  as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_B[\hat \lambda_0] \gets \bar \lambda_0^* - \hat \lambda_0$\;
 $\widehat {\bias}_B[\hat \lambda_1] \gets \bar \lambda_1^* - \hat \lambda_1$\;
 $(T_{\lambda_0}(\bar F^*), T_{\lambda_1}(\bar F^*)) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z_{B, \textrm{all}}, \vect u_{B, \textrm{all}})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \gets \bar \lambda_0^* - T_{\lambda_0}(\bar F^*)$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \gets \bar \lambda_1^* - T_{\lambda_1}(\bar F^*)$\;
 \SetArgSty{textrm}
 \Return{$\widehat {\mathrm{\sd}}_B[\hat \lambda_0], \widehat {\sd}_B[\hat \lambda_1],  \widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1],\widehat {\bias}_B[\hat \lambda_0], \widehat {\bias}_B[\hat \lambda_1], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$}z
 \caption{Bootstrap algorithm used in task C3}
 \label{bootstrap_algorithm}
\end{algorithm}

In Algorithm \ref{bootstrap_algorithm} it is presented pseudocode for a bootstrap algorithm for calculating estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Below it is implemented in code.


```{r}
# fast implementation of the EM algorithm that only stores and returns
# the final values of lambda0 and lambda1. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
fast.em.algorithm = function(z, u, lambdas.initial, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # Create variable for storing previous iteration value of lambda0 and lambda1 values for each iteration
  lambda0.prev = lambdas.initial[1]
  lambda1.prev = lambdas.initial[2]
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0 = n / sum(u * z + (1 - u) * (1 / lambda0.prev - z / (exp(lambda0.prev * z) - 1) )
    )
    lambda1 = n / sum((1 - u) * z + u * (1 / lambda1.prev - z / (exp(lambda1.prev * z) - 1) )
    )
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0 - lambda0.prev), abs(lambda1 - lambda1.prev))
    
    # update previous iteration value of lamba0 and lambda1 with newly calculated values
    lambda0.prev = lambda0
    lambda1.prev = lambda1
  }
  # return final estimates for lambda0 and lambda1
  return ( c(lambda0, lambda1))
}
```


```{r}
# function for performing what is described in the bootstrap algorithm
bootstrap.lambdas = function(z, u, B = 1000, initial.lambda0 = 1, initial.lambda1 = 1, epsilon = 1e-6) {
  "
  Input:
  - z: observed data as defined earlier (max of x_i and y_i, for i = 1,...,n)
  - u: observed data as defined earlier (1 if x_i >= y_i else 0, for i = 1, ...,n)
  - B: number of bootstrapped samples to use
  - initial.lambda0: initial value for lambda0 for use in em algorithm
  - initial.lambda1 initial value for lambda1 for use in em algorithm
  Output:
  - A list containing
    -- lambda0.est: maximum likelihood estimate for lambda0
    -- lambda1.est: maximum likelihood estimate for lambda1
    -- lambda0.bootstrap.mean: mean of maximum likelihood estimators for lambda0 based
    on bootstrapped data.
    -- lambda1.bootstrap.mean: mean of maximum likelihood estimators for lambda1 based
    on bootstrapped data.
    -- lambda0.bias.est: estimate of bias for lambda0 estimate
    -- lambda1.bias.est: estimate of bias for lambda1 estimate
    -- lambda0.bias.est.improved: improvedestimate of bias for lambda0 estimate
    -- lambda1.bias.est.improved: improvd estimate of bias for lambda1 estimate
    -- corr.lambda0.lambda1: esimate for the correlation between lambda0 and lambda1 estimators 
  "
  
  
  # initialize matrix for store bootstrapped lambda0 and lambda1 estimates
  lambdas.bootstrap = matrix(NA, ncol = 2, nrow = B+1)
  # the first row is only used for initializing the algorithm and will later be removed
  lambdas.bootstrap[1,] = c(initial.lambda0, initial.lambda1)
  n = length(z)
  # initialize vectors for storing all bootstraped data
  z.bootstrap.all = rep(NA, B*n)
  u.bootstrap.all = rep(NA, B*n)
  for (i in 1:B) {
    # draw which observations should be used
    drawn.obs = sample(1:n, n, replace=TRUE)
    # create new bootstrapped datasets
    z.bootstrap = z[drawn.obs]
    u.bootstrap = u[drawn.obs]
    # using the bootstrapped data, calculate the maximum likelihood estimates of
    # lambda0 and lambda1 using em algorithm
    lambdas.bootstrap[i+1,] = fast.em.algorithm(z.bootstrap, u.bootstrap,
                                                lambdas.bootstrap[i,], epsilon)
    # store the bootstrapped datasets
    z.bootstrap.all[(1+n*(i-1)):(n*i)] = z.bootstrap
    u.bootstrap.all[(1+n*(i-1)):(n*i)] = u.bootstrap
    
  }
  # removing the initial values from the lambdas matrix
  lambdas.bootstrap = lambdas.bootstrap[-1,]
  
  # calculating the mean of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.mean = apply(lambdas.bootstrap, 2, mean)
   # calculating the standard deviation of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.sd = apply(lambdas.bootstrap, 2, sd)
  
  # calculate estimates for lambda0 and lambda1 using original observed data
  lambdas.est = fast.em.algorithm(z, u, lambdas.bootstrap.mean, epsilon)
  
    # calculate T(\bar F^*) for use to calculate improved bias estimates
  lambdas.est.from.mean.cumulative.dist = fast.em.algorithm(z.bootstrap.all, u.bootstrap.all,
                                                            c(1,1), epsilon)
  
  return(list(lambda0.est = lambdas.est[1],
              
              lambda1.est = lambdas.est[2],
              
              lambda0.bootstrap.mean = lambdas.bootstrap.mean[1],
              
              lambda1.bootstrap.mean = lambdas.bootstrap.mean[2],
              
              lambda0.bias.est = lambdas.bootstrap.mean[1] - lambdas.est[1],
              
              lambda1.bias.est = lambdas.bootstrap.mean[2] - lambdas.est[2],
              
              lambda0.bias.est.improved = lambdas.bootstrap.mean[1] -
                
                lambdas.est.from.mean.cumulative.dist[1],
              
              lambda1.bias.est.improved = lambdas.bootstrap.mean[2] -
                
                lambdas.est.from.mean.cumulative.dist[2],
              
              lambda0.bootstrap.sd = lambdas.bootstrap.sd[1],
              
              lambda1.bootstrap.sd = lambdas.bootstrap.sd[2],
              
              corr.lambda0.lambda1 = corr(lambdas.bootstrap)
              ))
}

set.seed(1)
result.bootstrap = bootstrap.lambdas(z, u, B = 2000, epsilon = 1e-6)
```

In Table \ref{table:bootstrap_output} it is listed the standard deviations, biases and correlation between $(\hat \lambda_0, \hat \lambda_1)$. All the estimates are calculated using Algorithm \ref{bootstrap_algorithm}. Both of the standard deviation estimates and the correlation estimate appear reasonable. We observe that there is hardly any correlation between the two estimators $\hat \lambda_0$ and $\hat \lambda_1$.

\begin{table}[!h]
\centering
\begin{tabular}{| l | r |}
\hline
Variable & Bootstrap estimate \\
\hline
$\widehat {\mathrm{\sd}}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bootstrap.sd,4)` \\
$\widehat {\sd}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bootstrap.sd,4)` \\
$\widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1]$ & `r round(result.bootstrap$corr.lambda0.lambda1,4)` \\
$\widehat {\bias}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est,4)`\\ 
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est.improved,4)`\\
$\widehat {\bias}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est,4)`\\
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est.improved,4)`\\
\hline
\end{tabular}
\caption{Table containing the output from Algorithm \ref{bootstrap_algorithm} with $(\vect z, \vect u)$ as input observed data, $(\lambda_0^{(0)}, \lambda_1^{(0)}) = (1,1)$ and $B = 2000$.}
\label{table:bootstrap_output}
\end{table}

From Table \ref{table:bootstrap_output} we see that the bias estimates for $\hat \lambda_0$ is very similar for both methods for estimating bias. For $\hat \lambda_1$, however, the improved bias estimate is around twice as large as the other bias estimate. We choose to rely on the improved bias estimate. Consequently, the bias corrected estimates for $\lambda_0$ and $\lambda_1$ are

\begin{align*}
\hat \lambda_0^c &= \hat \lambda_0 - \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \\
&= `r round(result.em$lambda0, 4)` - `r round(result.bootstrap$lambda0.bias.est.improved,4)` \\
&= `r round(result.em$lambda0 - result.bootstrap$lambda0.bias.est.improved,4)` \\
\hat \lambda_1^c &= \hat \lambda_1 - \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \\
&= `r round(result.em$lambda1, 4)` - `r round(result.bootstrap$lambda1.bias.est.improved,4)` \\
&= `r round(result.em$lambda1 - result.bootstrap$lambda1.bias.est.improved,4)`
\end{align*}

As the bias estimates are quite small compared to the estimates of $\lambda_0$ and $\lambda_1$ we prefer the maximum likelihood estimates $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 4)`, `r round(result.em$lambda1, 4)`)$ over the bias corrected estimates $(\hat \lambda_0^c, \hat \lambda_1^c) = (`r round(result.em$lambda0 - result.bootstrap$lambda0.bias.est.improved,4)`, `r round(result.em$lambda1 - result.bootstrap$lambda1.bias.est.improved,4)`)$ since $\Var[\hat \lambda_0^c] \geq \Var[\hat \lambda_0]$ and $\Var[\hat \lambda_1^c] \geq \Var[\hat \lambda_1]$.



## 4)

An analytical formula for $f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1)$ can be derived as

\begin{align*}
f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1) 
&= u_i \cdot P(x_i = z_i | \lambda_0) \cdot P(y_i \leq z_i | \lambda_1) 
+ (1- u_i) P(x_i \leq z_i | \lambda_0) \cdot P(y_i = z_i | \lambda_0) \\
&= u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left[ \int_0^{z_i} \lambda_1 e^{- \lambda_1 y_i} dy_i \right]
+ (1 - u_i) \cdot \left[ \int_0^{z_i} \lambda_0 e^{- \lambda_0 x_i} dx_i\right] \cdot \lambda_1 e^{-\lambda_1 z_i} \\
&= u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left( 1 - e^{- \lambda_1 z_i} \right)
+ (1 - u_i) \cdot \left(1 - \lambda_0 e^{- \lambda_0 z_i} \right) \cdot \lambda_1 e^{-\lambda_1 z_i}
\end{align*}

The likelihood of $(\lambda_0, \lambda_1)$ given the observed data $(\vect z, \vect u)$ is

\begin{align*}
L(\lambda_0, \lambda_1 | \vect z, \vect u) &= f_{\vect Z, \vect U}(\vect z, \vect u | \lambda_0, \lambda_1) \\
&= \prod_{i=1}^n f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1) \\
&= \prod_{i=1}^n \left[
u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left( 1 - e^{- \lambda_1 z_i} \right)
+ (1 - u_i) \cdot \left(1 - \lambda_0 e^{- \lambda_0 z_i} \right) \cdot \lambda_1 e^{-\lambda_1 z_i}
\right]
\end{align*}

Since either $u_i = 1 \land (1-u_i) = 0$ or $u_i = 0 \land (1-u_i) = 1$ we have that the log likelihood of $(\lambda_0, \lambda_1)$ given the observed data $(\vect z, \vect u)$ is

\begin{align*}
l(\lambda_0, \lambda_1 | \vect z, \vect u) &=
\log(L(\lambda_0, \lambda_1 | \vect z, \vect u)) \\
&= \sum_{i=1}^n 
\left( 
u_i \left[ \ln(\lambda_0) - \lambda_0 z_i + \ln(1-e^{- \lambda_1 z_i}) \right]
+ (1 - u_i) \left[ \ln(1-e^{- \lambda_0 z_i}) + \ln(\lambda_1) - \lambda_1 z_i \right]
\right)
\end{align*}

The maximum likelihood estimators (MLE) are $(\hat \lambda_0, \hat \lambda_1) =\argmax_{(\lambda_0, \lambda_1)} = L(\lambda_0, \lambda_1 | \vect z, \vect u) = \argmax_{(\lambda_0, \lambda_1)} l(\lambda_0, \lambda_1 | \vect z, \vect u)$. Unfortunately, it is not possible to find analytical formulas for the maximum likelihood estimators. We can, however, find the MLE numerically. An advantage of optimizing the likelihood directly compared to the EM algorithm is that when optimizing the likelihood directly the standard deviations of $\hat \lambda_0, \hat \lambda_1$ and the correlation between them is directly available without bootstrapping.

Let $\hat {\vect \lambda} = [\hat \lambda_0, \hat \lambda_1]^T$. To get an estimate for $\Cov [\hat {\vect \lambda}]$ we use that for independent samples we have that asymptotically as the number of samples goes to infinity the maximum likelihood estimates have the distribution

\begin{equation*}
\hat {\vect \lambda} \sim \mathcal{N}(\vect \lambda, H(\vect \lambda)^{-1})
\end{equation*}

where $H$ is the observed Fisher matrix

\begin{equation*}
H(\vect \lambda) = - \frac{\partial^2}{\partial \vect \lambda \partial \vect \lambda^T} l(\vect \lambda | \vect z, \vect u)
\end{equation*}

?????????????? Must we talk about or use expected Fisher matrix?

Below we optimize the log likelihood function $l(\lambda_0, \lambda_1 | \vect z, \vect u)$ and use that $H(\vect \lambda) \approx H(\hat {\vect \lambda})$ to estimate standard deviations and correlations between the estimators $(\hat \lambda_0, \hat \lambda_1)$.

```{r}
# Create the log likelihood function that shall be optimized with respect to lambda0 and lambda1
log.likelihood = function(lambdas, z, u) {
  return(sum(u * (log(lambdas[1]) - lambdas[1] * z + log(1 - exp(-lambdas[2] * z)))
             + (1-u) * (log(1 - exp(-lambdas[1] * z)) + log(lambdas[2]) - lambdas[2] * z) ) )
}


initial.lambdas = c(1,1) # for initializaing the numerical search
set.seed(1) # set seed for reproducible results
# optimize the log likelihood function with respect to lambda0 and lambda1
# also calculate the Hessian matrix at the optimal values for lambda0 and lambda1
# use fnscale = -1 for maximization instead of minimization
# limit lambda0 and lambda1 to be larger than 0
optim.result = optim(initial.lambdas, log.likelihood, NULL, z, u, control = list(fnscale = -1), hessian=TRUE, lower=c(0,0))
# calculate the mle estimate of the covariance matrix of the estimators for [lambda0, lambda1]
cov.lambdas.mle = solve(- optim.result$hessian)
# print the mle estimate of the covariance matrix of the estimator for [lambda0, lambda1]
cov.lambdas.mle

# calculate standard deviations and correlation between lambda0 and lambda1 estimators
lambda0.sd.mle = sqrt(cov.lambdas.mle[1,1])
lambda1.sd.mle = sqrt(cov.lambdas.mle[2,2])
corr.lambda0.lambda1.mle = cov.lambdas.mle[1,2] / (lambda0.sd.mle * lambda1.sd.mle)
```

\begin{table}[!h]
\centering
\begin{tabular}{| l | r | r|}
\hline
Variable & EM estimate & MLE estimate \\
\hline
$ \hat \lambda_0$ & `r round(result.em$lambda0,4)` & `r round(optim.result$par[1],4)` \\
$ \hat \lambda_1$ & `r round(result.em$lambda1,4)` & `r round(optim.result$par[2],4)` \\
\hline
\end{tabular}
\caption{Comparison of estimates for $\lambda_0$ and $\lambda_1$ computed with Expectation-Maximization (EM) algorithm or direct maximization of likelihood function.}
\label{table:em_mle_comparison}
\end{table}

In Table \ref{table:em_mle_comparison} we compare the estimates from the EM-algorithm against the estimates from direct numerical maximization of the log-likelihood function. The estimates from both methods are very similar, in fact they are identical to at least 4 decimals accuracy. This is to be expected as they both are based on numerical maximization of the likelihood function.

\begin{table}[!h]
\centering
\begin{tabular}{| l | r | r|}
\hline
Variable & Bootstrap estimate & MLE estimate \\
\hline
$\widehat {\mathrm{\sd}}[\hat \lambda_0]$ & $`r round(result.bootstrap$lambda0.bootstrap.sd,4)`$ & 
`r round(lambda0.sd.mle,4)`\\
$\widehat {\sd}[\hat \lambda_1]$ & $`r round(result.bootstrap$lambda1.bootstrap.sd,4)`$ &
`r round(lambda1.sd.mle,4)`\\
$\widehat {\Corr}[\hat \lambda_0, \hat \lambda_1]$ & $`r round(result.bootstrap$corr.lambda0.lambda1,4)`$ &
`r round(corr.lambda0.lambda1.mle,4)`\\
\hline
\end{tabular}
\caption{Comparison of estimates for standard deviations of and correlation between $\hat \lambda_0$ and $\hat \lambda_1$ computed with bootstraping or direct maximization of likelihood function.}
\label{table:bootstrap_mle_comparison}
\end{table}

In Table \ref{table:bootstrap_mle_comparison} we compare estimates of standard deviations and correlation between the bootstrapping method and through direct maximization of likelihood function. The estimates from the two methods are quite similar, but as expected they differ a little. We conclude that both methods appear to give acceptable results.