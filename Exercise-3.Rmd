---
title: "Excercise 3"
subtitle: "TMA4300 Computer intensive statistical methods Spring 2021"
author: 
- "Johan Fredrik Agerup"
- "Arne Rustad"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
   - \usepackage{subfig}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(boot)
library(latex2exp)
library(coda)
library(dplyr)
library(tidyverse)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

```{r, fig.height=3, fig.width=4}
source("probAdata.R")
plot(data3A$x)
```
We consider an AR(2) model which is specified by the relation:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t
\end{equation}

The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\mathbf{\beta}$:

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|
\end{align}

Denote the minimisers by $\mathbf{\beta}_{LS}$ and $\mathbf{\beta}_{LA}$ (calculated by ARp.beta.est), and define the estimated residuals to be :
\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t -\hat{\beta}_1x_{t-1} -\hat{\beta}_2 x_{t-2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. (Results for $\hat{\epsilon}_t$ obtained by LS and LA can be calculated with ARp.resid).

## A.1

### 1.1 Use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators.

```{r}
source("probAhelp.R", local = knitr::knit_global())
minisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)
eps_residuals_LS = ARp.resid(x = data3A$x, beta = minisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minisers_ar2$LA)

#initialise values for x1 and x2 
#by picking a random consecutive subsequence from the data
sample_consecutive = function(x){
  #set seed for reproducability
  set.seed(2021)
  x_reshaped = matrix(unlist(t(x)), byrow=T, 50, 2)
  idx = sample(seq(1,50), size = 1, replace = FALSE)
  x_sample_pair = x_reshaped[idx,]
  return(x_sample_pair)
}

x1x2_sample1 = sample_consecutive(x = data3A$x)
print(x1x2_sample1)
```
To do a resampling, initialise values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data.
```{r, fig.height=3, fig.width=4}
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=length(eps_residuals_LS), replace=TRUE))
plot(calc_x_LS)
```

```{r, fig.height=3, fig.width=4}
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=length(eps_residuals_LA), replace=TRUE))
plot(calc_x_LA)
```

### 1.2 Specifically, estimate the variance and bias of the two estimators.

```{r}

"
Agenda:
- implement theta_hat*(b) which is the empirical variance of the bootstrapsample for every t
  - and for b = 1,2,...,B
- Estimate the empirical SD(theta_hat*) by the formula for SE_B^
- compute theta_hat*(dot)

"

# bootstrap resampling
residual_bootstrap_resampling = function(x, p, B){
  samples_LS = c()
  samples_LA = c()
  sd_LS = c()
  sd_LA = c()
  for(i in 1:B){
    " Done equally for both methods LS, LA
    Algo:
    1. Sample pair x1 and x2 from the given data
    2. Use help function ARp.beta.est to calculate the betas
    3. Use help function ARp.resid to calc residuals
    4. Use help function ARp.filter to resample w/ replacement
    5. Add sd for current sample to list of sds
    6. Calc the estimator for the SD
    "
    
    x_pair = sample_consecutive(x = x)
    beta_vecs = ARp.beta.est(x = x, p = p)
    eps_LS = ARp.resid(x = x, beta = beta_vecs$LS)
    eps_LA = ARp.resid(x = x, beta = beta_vecs$LA)
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LS, 
                         e = sample(eps_LS, size=length(eps_LS), replace=TRUE))
    sd_LS = c(sd_LS, sqrt(var(x_LS)))
    # print(sqrt(var(x_LS)))
    samples_LS = c(samples_LS, x_LS)
    
    
    x_LA = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LA, 
                         e = sample(eps_LA, size=length(eps_LA), replace=TRUE))
    sd_LA = c(sd_LA, sqrt(var(x_LA)))
    samples_LA = c(samples_LA, x_LA)
  }
  hat_SE_B_LS = sqrt(sum((sd_LS - mean(sd_LS)/B)^2)/(B -1))
  hat_SE_B_LA = sqrt(sum((sd_LA - mean(sd_LA)/B)^2)/(B -1))
  df_boot = data.frame(LS = samples_LS, LA = samples_LA, 
                       est_sd_LS = hat_SE_B_LS,
                       est_sd_LA = hat_SE_B_LA)
  return(df_boot)
}

#estimating  the variance and bias of the two estimators
boot1 = residual_bootstrap_resampling(x = data3A$x, p = 2, B = 2000)
# hist(data3A$x)
# hist(boot1$LS)
# hist(boot1$LA)
# var_boot1_LS = var(boot1$LS)
# print(var_boot1_LS)
# var_boot1_LA = var(boot1$LA)
# print(var_boot1_LA)

#computing bias:
bias_LS = boot1$est_sd_LS[1] - mean(data3A$x)
bias_LA = boot1$est_sd_LA[1] - mean(data3A$x)
print(bias_LS)
print(bias_LA)
#estimating variance and bias of the two estimators after bootstrap
```


The LS estimator is optimal for Gaussian AR(p) processes. 

### Explain if it is also optimal for this problem.






# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. $Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i) \qquad \textrm{for } i = 1, \dots, n $ we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f(\vect x | \lambda_0) f(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f(y_i | \lambda_1) \right ]
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{equation*}
\begin{split}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \ln \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{split}
\end{equation*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $(\lambda_o^{(t)}, \lambda_1^{(t)})$.

\begin{equation} \label{eq:em_formula_incomplete}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E [x_i | z_i, u_i, \lambda_0^{(t)}]  - \lambda_1 \sum_{i=1}^n \E [y_i | z_i, u_i, \lambda_1^{(t)}
\end{split}
\end{equation}

For the case $u_i = 1$ then we have

\begin{align*}
\E [x_i | z_i, u_i = 1, \lambda_0^{(t)}] &= u_i
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 1, \lambda_1^{(t)}
&= \int_0^{z_i} y_i P \left( y_i | y_i \leq z_i, \lambda_1^{(t)}\right) dy_i \\
&=\int_0^{z_i} y_i \frac{P \left( y_i \cap y_i \leq z_i | \lambda_1^{(t)} \right) }
{P \left( y_i \leq z_i | \lambda_1^{(t)} \right) } dy_i \\
&= \int_0^{z_i} y_i \frac{f(y_i | \lambda_1^{(t)})}{ \int_0^{z_i} f(y_i^* | \lambda_1^{(t)}) dy_i^*} dy_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i^* } dy_i^*}
\int_0^{z_i} y_i \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i} dy_i) \\
&= \frac{1}{1 - e^{- \lambda_1^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_1^{(t)} z_i} - \frac{1}{\lambda_1^{(t)}} e^{- \lambda_1^{(t)} z_i} + \frac{1}{\lambda_1^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_1^{(t)} z_i}}{1 - e^{- \lambda_1^{(t)} z_i}} + \frac{1}{\lambda_1^{(t)}} \\
&= \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
\end{align*}

Correspondlingly, we have for the $u_i = 0$ that

\begin{align*}
\E [x_i | z_i, u_i = 0, \lambda_0^{(t)}
&= \int_0^{z_i} x_i P \left( x_i | x_i \leq z_i, \lambda_0^{(t)}\right) dx_i \\
&=\int_0^{z_i} x_i \frac{P \left( x_i \cap x_i \leq z_i | \lambda_0^{(t)} \right) }
{P \left( x_i \leq z_i | \lambda_0^{(t)} \right) } dx_i \\
&= \int_0^{z_i} x_i \frac{f(x_i | \lambda_0^{(t)})}{ \int_0^{z_i} f(x_i^* | \lambda_0^{(t)}) dx_i^*} dx_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i^* } dx_i^*}
\int_0^{z_i} x_i \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i} dx_i) \\
&= \frac{1}{1 - e^{- \lambda_0^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_0^{(t)} z_i} - \frac{1}{\lambda_0^{(t)}} e^{- \lambda_0^{(t)} z_i} + \frac{1}{\lambda_0^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_0^{(t)} z_i}}{1 - e^{- \lambda_0^{(t)} z_i}} + \frac{1}{\lambda_0^{(t)}} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 0, \lambda_1^{(t)}] &= u_i \\
\end{align*}

Inserting this result into Equation (\ref{eq:em_formula_incomplete}) we get

\begin{equation} \label{eq:em_formula}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
=& \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
\big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
=& n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) \\
&- \lambda_0 \sum_{i=1}^n 
\left [ u_i z_i 
+ (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1} \right)
\right] \\
&- \lambda_1 \sum_{i=1}^n \left [
(1 - u_i) z_i + 
u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1} \right)
\right ]
\end{split}
\end{equation}

## 2)

Let $Q(\lambda_0, \lambda_1)$ be defined

\begin{equation*}
Q(\lambda_0, \lambda_1)  = 
= Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)})
= \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
\end{equation*}
