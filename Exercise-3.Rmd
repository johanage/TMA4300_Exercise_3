---
title: "Excercise 3"
subtitle: "TMA4300 Computer intensive statistical methods Spring 2021"
author: 
- "Johan Fredrik Agerup"
- "Arne Rustad"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
   - \usepackage{subfig}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(latex2exp)
library(coda)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

```{r, fig.height=3, fig.width=4}
source("probAdata.R")
plot(data3A$x)
```
We consider an AR(2) model which is specified by the relation:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t
\end{equation}

The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\mathbf{\beta}$:

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|
\end{align}

Denote the minimisers by $\mathbf{\beta}_{LS}$ and $\mathbf{\beta}_{LA}$ (calculated by ARp.beta.est), and define the estimated residuals to be :
\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t −\hat{\beta}_1x_{t−1} −\hat{\beta}_2 x_{t−2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. (Results for $\hat{\epsilon}_t$ obtained by LS and LA can be calculated with ARp.resid).

## A.1

### 1.1 Use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators.

```{r}
source("probAhelp.R")
minisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)

eps_residuals_LS = ARp.resid(x = data3A$x, beta = minisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minisers_ar2$LA)

#initialise values for x1 and x2 
#by picking a random consecutive subsequence from the data
sample_consecutive = function(x){
  #set seed for reproducability
  set.seed(2021)
  x_reshaped = matrix(unlist(t(x)), byrow=T, 50, 2)
  idx = sample(seq(1,50), size = 1, replace = FALSE)
  print(idx)
  x_sample_pair = x_reshaped[idx,]
  return(x_sample_pair)
}

x1x2_sample1 = sample_consecutive(x = data3A$x)
print(x1x2_sample1)
```
To do a resampling, initialise values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data.
```{r, fig.height=3, fig.width=4}
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=100, replace=TRUE))
plot(calc_x_LS)
```

```{r, fig.height=3, fig.width=4}
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=100, replace=TRUE))
plot(calc_x_LA)
```

### 1.2 Specifically, estimate the variance and bias of the two estimators.

```{r}
# bootstrap resampling
residual_bootstrap_resampling = function(x, p, B){
  samples_LS = matrix(data = NA, nrow = 1500, ncol = 100)
  samples_LA = matrix(data = NA, nrow = 1500, ncol = 100)
  for(i in range(B)){
    x_pair = sample_consecutive(x = x)
    beta_vecs = ARp.beta.est(x = x, p = p)
    eps_LS = ARp.resid(x = x, beta = beta_vecs$LS)
    eps_LA = ARp.resid(x = x, beta = beta_vecs$LA)
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LS, 
                         e = sample(eps_LS, size=100, replace=TRUE))
    samples_LS[i,] = x_LS
    x_LA = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LA, 
                         e = sample(eps_LA, size=100, replace=TRUE))
    samples_LA[i,] = x_LA
    
  }
  df_boot = data.frame(LS = samples_LS, LA = samples_LA)
  return(df_boot)
}

boot1 = residual_bootstrap_resampling(x = data3A$x, p = 2, B = 1500)
#estimating variance and bias of the two estimators after bootstrap
```


The LS estimator is optimal for Gaussian AR(p) processes. 

### Explain if it is also optimal for this problem.

# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. $Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i) \qquad \textrm{for } i = 1, \dots, n $ we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f(\vect x | \lambda_0) f(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f(y_i | \lambda_1) \right ]
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \log \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \log(\lambda_0) + \log(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{align*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $\lambda_o^{(t)}, \lambda_1^{(t)}$.

\begin{align*}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \log\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \log(\lambda_0) + \log(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E [x_i | z_i, u_i, \lambda_0^{(t)}]  - \lambda_1 \sum_{i=1}^n \E [y_i | z_i, u_i, \lambda_1^{(t)}
\end{align*}