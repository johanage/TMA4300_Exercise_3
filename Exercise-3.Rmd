---
title: "Excercise 3"
subtitle: "TMA4300 Computer intensive statistical methods Spring 2021"
author: 
- "Johan Fredrik Agerup"
- "Arne Rustad"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
   - \usepackage{subfig}
   - \usepackage[ruled,vlined]{algorithm2e}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\sd}{\textrm{SD}}
\newcommand{\bias}{\textrm{Bias}}


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(boot)
library(latex2exp)
library(coda)
library(dplyr)
library(tidyverse)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

```{r, fig.height=3, fig.width=4}
source("probAdata.R")
plot(data3A$x)
```
We consider an AR(2) model which is specified by the relation:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t
\end{equation}

The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\mathbf{\beta}$:

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|
\end{align}

Denote the minimisers by $\mathbf{\beta}_{LS}$ and $\mathbf{\beta}_{LA}$ (calculated by ARp.beta.est), and define the estimated residuals to be :
\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t -\hat{\beta}_1x_{t-1} -\hat{\beta}_2 x_{t-2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. (Results for $\hat{\epsilon}_t$ obtained by LS and LA can be calculated with ARp.resid).

## A.1

### 1.1 Use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators.

```{r}
source("probAhelp.R", local = knitr::knit_global())
minisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)
eps_residuals_LS = ARp.resid(x = data3A$x, beta = minisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minisers_ar2$LA)

#initialise values for x1 and x2 
#by picking a random consecutive subsequence from the data

"Agenda:
- det er ikke bare 50 mulige par, men 99
- eks: x1x2, x2x3, x3x4, ..."
sample_consecutive = function(x){
  #set seed for reproducability
  #set.seed(2021)
  x_reshaped = matrix(unlist(t(x)), byrow=T, 50, 2)
  idx = sample(seq(1,50), size = 1, replace = FALSE)
  x_sample_pair = x_reshaped[idx,]
  return(x_sample_pair)
}

x1x2_sample1 = sample_consecutive(x = data3A$x)
print(x1x2_sample1)
```
To do a resampling, initialise values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data.
```{r, fig.height=3, fig.width=4}
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=length(eps_residuals_LS), replace=TRUE))
plot(calc_x_LS)
```

```{r, fig.height=3, fig.width=4}
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=length(eps_residuals_LA), replace=TRUE))
plot(calc_x_LA)
```

### 1.2 Specifically, estimate the variance and bias of the two estimators.

```{r}

"
Agenda:
- implement theta_hat*(b) which is the empirical variance of the bootstrapsample for every t
  - and for b = 1,2,...,B
- Estimate the empirical SD(theta_hat*) by the formula for SE_B^
- compute theta_hat*(dot)
- estimate the variance and the bias of the estimators, but what is the estimators here?
  - is it beta1 and beta2
  - is it epsilon_LA and epsilon_LS, aka the residuals of the two lossfunction definitions
"

# bootstrap resampling
residual_bootstrap_resampling = function(x, p, B){
  samples_LS = c()
  samples_LA = c()
  beta_est_LS = c()
  beta_est_LA = c()
  beta_vec_OG = ARp.beta.est(x = x, p = p)
  eps_LS = ARp.resid(x = x, beta = beta_vec_OG$LS)
  eps_LA = ARp.resid(x = x, beta = beta_vec_OG$LA)
  for(i in 1:B){
    " Done equally for both methods LS, LA
    Algo:
    1. Sample pair x1 and x2 from the given data
    2. Use help function ARp.beta.est to calculate the betas
    3. Use help function ARp.resid to calc residuals
    4. Use help function ARp.filter to resample w/ replacement
    5. Add sd for current sample to list of sds
    6. Calc the estimator for the SD
    "
    #sample pair from original time series data
    x_pair = sample_consecutive(x = x)
    
    # generate beta ector for LS and LA
    # using minimiserts from the bootstrapped data
    "Questions:
    - use same betas for every bootstrap or use the bootstrap data to minimize loss functions?"
    
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vec_OG$LS, 
                         e = sample(eps_LS, size=length(eps_LS), replace=TRUE))
    beta_vec_star = ARp.beta.est(x = x_LS, p = p)
    beta_est_LS = c(beta_est_LS, beta_vec_star$LS)
    # print(sqrt(var(x_LS)))
    samples_LS = c(samples_LS, x_LS)
    
    
    
    x_LA = ARp.filter(x0 = x_pair, 
                         beta = beta_vec_OG$LA, 
                         e = sample(eps_LA, size=length(eps_LA), replace=TRUE))
    samples_LA = c(samples_LA, x_LA)
    beta_vec_star = ARp.beta.est(x = x_LA, p = p)
    beta_est_LA = c(beta_est_LA, beta_vec_star$LA)
  }
  hat_SE_B_LS = sum((beta_est_LS - mean(beta_est_LS))^2)/(B -1)
  hat_SE_B_LA = sum((beta_est_LA - mean(beta_est_LA))^2)/(B -1)
  df_boot = data.frame(LS = samples_LS, LA = samples_LA, 
                       est_sd_beta_LS = hat_SE_B_LS,
                       est_sd_beta_LA = hat_SE_B_LA,
                       bias_beta_LS = hat_SE_B_LS - mean(beta_est_LS),
                       bias_beta_LA = hat_SE_B_LA - mean(beta_est_LA))  
  return(df_boot)
}

#estimating  the variance and bias of the two estimators
boot1 = residual_bootstrap_resampling(x = data3A$x, p = 2, B = 2000)
print(paste0("Variance of the betas: LS = ", boot1$est_sd_beta_LS[1]," and LA = ", boot1$est_sd_beta_LA[1]))

#computing bias:
print("Bias of the two estimators:")
print(boot1$bias_beta_LS[1])
print(boot1$bias_beta_LA[1])

```


The LS estimator is optimal for Gaussian AR(p) processes. 

### Explain if it is also optimal for this problem.



# Problem B Permutation Test

```{r}
bilirubin <- read.table("bilirubin.txt",header=T)
head(bilirubin)


"Use a boxplot to inspect the logarithms of the concentrations for each individual. Be careful to use the same y-axis to make the plots comparable. Use the function lm in R to fit the regression model log Yij = βi + epsilon_ij , with i = 1, 2, 3 and j = 1, . . . , ni (1)
where n1 = 11, n2 = 10 and n3 = 8, and epsilon_ij iid∼ N (0, σ2). Use the F-test to test the hypothesis that β1 = β2 = β3 and save the value of the F-statistic as Fval. Is the hypothesis accepted?"
model_bilirubin <- lm(log(meas)~pers,data=bilirubin)
F_statistic = summary.lm(model_bilirubin)

"Write a function permTest() which generates a permutation of the data between the three individuals, consequently fits the model given in (1) and finally returns the value of the F-statistic for testing β1 = β2 = β3."
permTest = function() {return(0)}


"Perform a permutation test using the function permTest to generate a sample of size 999 for the F-statistic. Compute the p-value for Fval using this sample. What do you observe?
"
```



# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. $Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i) \qquad \textrm{for } i = 1, \dots, n $ we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f(\vect x | \lambda_0) f(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f(y_i | \lambda_1) \right ]
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{equation*}
\begin{split}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \ln \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{split}
\end{equation*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $(\lambda_o^{(t)}, \lambda_1^{(t)})$.

\begin{equation} \label{eq:em_formula_incomplete}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E [x_i | z_i, u_i, \lambda_0^{(t)}]  - \lambda_1 \sum_{i=1}^n \E [y_i | z_i, u_i, \lambda_1^{(t)}
\end{split}
\end{equation}

For the case $u_i = 1$ then we have

\begin{align*}
\E [x_i | z_i, u_i = 1, \lambda_0^{(t)}] &= u_i
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 1, \lambda_1^{(t)}
&= \int_0^{z_i} y_i P \left( y_i | y_i \leq z_i, \lambda_1^{(t)}\right) dy_i \\
&=\int_0^{z_i} y_i \frac{P \left( y_i \cap y_i \leq z_i | \lambda_1^{(t)} \right) }
{P \left( y_i \leq z_i | \lambda_1^{(t)} \right) } dy_i \\
&= \int_0^{z_i} y_i \frac{f(y_i | \lambda_1^{(t)})}{ \int_0^{z_i} f(y_i^* | \lambda_1^{(t)}) dy_i^*} dy_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i^* } dy_i^*}
\int_0^{z_i} y_i \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i} dy_i) \\
&= \frac{1}{1 - e^{- \lambda_1^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_1^{(t)} z_i} - \frac{1}{\lambda_1^{(t)}} e^{- \lambda_1^{(t)} z_i} + \frac{1}{\lambda_1^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_1^{(t)} z_i}}{1 - e^{- \lambda_1^{(t)} z_i}} + \frac{1}{\lambda_1^{(t)}} \\
&= \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
\end{align*}

Correspondlingly, we have for the $u_i = 0$ that

\begin{align*}
\E [x_i | z_i, u_i = 0, \lambda_0^{(t)}
&= \int_0^{z_i} x_i P \left( x_i | x_i \leq z_i, \lambda_0^{(t)}\right) dx_i \\
&=\int_0^{z_i} x_i \frac{P \left( x_i \cap x_i \leq z_i | \lambda_0^{(t)} \right) }
{P \left( x_i \leq z_i | \lambda_0^{(t)} \right) } dx_i \\
&= \int_0^{z_i} x_i \frac{f(x_i | \lambda_0^{(t)})}{ \int_0^{z_i} f(x_i^* | \lambda_0^{(t)}) dx_i^*} dx_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i^* } dx_i^*}
\int_0^{z_i} x_i \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i} dx_i) \\
&= \frac{1}{1 - e^{- \lambda_0^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_0^{(t)} z_i} - \frac{1}{\lambda_0^{(t)}} e^{- \lambda_0^{(t)} z_i} + \frac{1}{\lambda_0^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_0^{(t)} z_i}}{1 - e^{- \lambda_0^{(t)} z_i}} + \frac{1}{\lambda_0^{(t)}} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 0, \lambda_1^{(t)}] &= u_i \\
\end{align*}

Inserting this result into Equation (\ref{eq:em_formula_incomplete}) we get

\begin{equation} \label{eq:em_formula}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
=& \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
\big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
=& n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) \\
&- \lambda_0 \sum_{i=1}^n 
\left [ u_i z_i 
+ (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1} \right)
\right] \\
&- \lambda_1 \sum_{i=1}^n \left [
(1 - u_i) z_i + 
u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1} \right)
\right ]
\end{split}
\end{equation}

## 2)

Let $Q(\lambda_0, \lambda_1)$ be defined

\begin{equation*}
Q(\lambda_0, \lambda_1)
= Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)})
= \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
\end{equation*}

We want to use the Expectation-Maximization (EM) algorithm as defined in Algorithm \ref{em_algorithm} to find the maximum likelihood estimates of $(\lambda_0, \lambda_1)$ iteratively. We denote these estimates $(\hat \lambda_0, \hat \lambda_1)$.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Function $l(\lambda_0, \lambda_1 | \vect x, \vect y)$ and starting values $\lambda_0^{(0)}, \lambda_1^{(0)} $}
 \KwOut {Maximum likelihood estimates for $(\lambda_0, \lambda_1)$.}
 $t \gets 0$\;
 \While{not converged}{
  $Q(\lambda_0, \lambda_1) = \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]$ \;
  $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)}) = \argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1) $\;
    $t \gets t + 1$ \;
 }
 \Return{$(\hat \lambda_0, \hat \lambda_1) = (\lambda_0^{(t)}, \lambda_1^{(t)})$}
 \caption{EM algorithm to maximize $l(\lambda_0, \lambda_1 |  \vect z, \vect u)$}
 \label{em_algorithm}
\end{algorithm}

In Task C1 we found an expression for $Q(\lambda_0, \lambda_1)$. To be able to implement Algorithm \ref{em_algorithm} we need to find an expression for $\argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1)$. To maximize $Q(\lambda_0, \lambda_1)$ with respect to $(\lambda_0, \lambda_1)$ we set the partial derivatives equal to zero and solve for each of the parameters.

\begin{align}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_0} - \sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ] = 0 \nonumber \\
  \lambda_0 = \frac{n}
  {\sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda0}
\end{align}

\begin{align}
  \frac{\partial}{\partial \lambda_1} Q(\lambda_1, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_1} - \sum_{i=1}^n \left[ (1 - u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ] = 0
  \nonumber \\
  \lambda_1 = \frac{n}
  {\sum_{i=1}^n \left[ (1-u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda1}
\end{align}

To ensure we have found a maximum we also check the second derivatives

\begin{align*}
\frac{\partial^2}{\partial \lambda_0^2} &= \frac{-n}{\lambda_0^2} < 0\\
  \frac{\partial^2}{\partial \lambda_1^2} &= \frac{-n}{\lambda_1^2} < 0
\end{align*}

As the second derivatives are stricly negative for all $\lambda_0$ and $\lambda_1$ we know that we have found the values that maximizes $Q(\lambda_0, \lambda_1)$. From Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) we have a recursive expression for the maximum likelihood estimates of $(\lambda_0, \lambda_1)$.

Below we implement the EM-algorithm for this specific case and find the maximum likelihood estimates when the data is as specified in the files \textbf{z.txt} and \textbf{u.txt} available from the course home page. As stopping criteria we use that the maximum absolute value parameter change from one iteration to the next must be smaller than some chosen $\epsilon$, that is 
$\max \left \{ |\lambda_0^{(t+1)} - \lambda_0^{(t)}|, |\lambda_1^{(t+1)} - \lambda_1^{(t)}| \right \} < \epsilon$.

```{r}
# implementation of the EM algorithm. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
em.algorithm = function(z, u, lambda0, lambda1, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # create vectors for storing lambda0 and lambda1 values for each iteration
  lambda0.vec = c(lambda0)
  lambda1.vec = c(lambda1)
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  t = 1
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0.vec = c(lambda0.vec, 
                       n / sum(u * z + (1 - u) * (1 / lambda0.vec[t] - z / (exp(lambda0.vec[t] * z) - 1) ) )
    )
    lambda1.vec = c(lambda1.vec,
                    n / sum((1 - u) * z + u * (1 / lambda1.vec[t] - z / (exp(lambda1.vec[t] * z) - 1) ) )
    )
    t = t + 1
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0.vec[t] - lambda0.vec[t-1]), abs(lambda1.vec[t] - lambda1.vec[t-1]))
  }
  # get the number of iterations
  n.iter = t-1
  # create a data frame for storing parameter values for each iteration for easy plotting
  df = data.frame(iter = seq(0,n.iter), lambda0 = lambda0.vec, lambda1 = lambda1.vec)
  # same data frame on long format (also for easy plotting)
  df.long = pivot_longer(df, -iter, names_to = "Parameter")
  # return a list with useful values
  return ( list(lambda0 = lambda0.vec[t],lambda1 = lambda1.vec[t],
                lambda0.vec = lambda0.vec, lambda1.vec = lambda1.vec, n.iter = n.iter,
                df = df, df.long = df.long))
}
```


```{r}
# fetch the data
z = read.csv("z.txt")[,1]
u = read.csv("u.txt")[,1]
# find maximum likelihood estimates
result.em = em.algorithm(z, u, 1, 1)
```

The maximum likelihood estimates of $(\lambda_0, \lambda_1)$ are $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 3)`, `r round(result.em$lambda1, 3)`)$. Next we test the convergence of the algorithm for different initial values.

```{r, fig.width=8, fig.height = 3, fig.cap = "\\label{fig:convergence_plot} Visualization of convergence for each of the parameters $(\\lambda_0,\\lambda_1)$ for three different initial values $(\\lambda_0^{(0)}, \\lambda_1^{(0)}) \\in [(1,1), (7,7), (15, 15)]$. As stopping criteria we use $\\textrm{max} [ |\\lambda_0^{(t+1)} - \\lambda_0^{(t)}|, |\\lambda_1^{(t+1)} - \\lambda_1^{(t)}| ] < 10^{-6}$."}
# test for different initial values
# choose the initial value combinations
initial.lambda0 = c(1, 7, 15)
initial.lambda1 = c(1, 7, 15)

# create a data frame to store all the realizations
df.em.long = data.frame()
for (i in 1:length(initial.lambda0)) {
  result.em.i = em.algorithm(z, u, initial.lambda0[i], initial.lambda1[i])
  df.em.long = rbind(df.em.long, cbind(result.em.i$df.long, "initial.value" = toString(i)))
}
# visialize convergence for different initial values
ggplot(df.em.long, aes(x = iter, y = value, col = initial.value)) + geom_line() +
  
  facet_wrap(~Parameter) + guides(col = guide_legend("Initial values")) +
  
  scale_color_discrete(labels = c("(1,1)", "(7,7)", "(15,15)")) +
  
  ggtitle("Convergence plot of estimates")
```

From Figure \ref{fig:convergence_plot} we see that the algorithm quickly converges for all three initial conditions. The estimate for $\lambda_0$ appear to be almost converged after only a single iteration, while the estimate for $\lambda_1$ seem to need around 5 iterations for the same level of precicion. After 16 iterations all three initial positions were converged according to $\epsilon = 10^{-6}$.

## 3)

In this task we use bootstrapping to estimate the standard deviations and the biases of each of $\hat \lambda_0$ and $\hat \lambda_1$ along with an estimate for $\Corr [\hat \lambda_0, \hat \lambda_1]$. First we define som notation. We will bootstrap $B$ new datasets $(\vect z^{b*}, \vect u^{b*}), \; b = 1, \dots, B$. These datasets will each be used to calculate the two estimators of interest $(\hat \lambda_0^{b*}, \hat \lambda_1^{b*})$ using Algorithm \ref{em_algorithm} on $(\vect z^{b*}, \vect u^{b*})$. From the $B$ pairs of parameter estimates we can estimate the standard deviations of $\hat \lambda_0$ and $\hat \lambda_1$ in addition to the correlation between them.

\begin{align*}
  \widehat {\sd_{B}[\hat \lambda_0]} &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)^2} \\
  \widehat {\sd_{B}[\hat \lambda_1]} &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_1^{b*} -\bar \lambda_1^*)^2} \\
  \widehat {\Corr_{B}[\hat \lambda_0, \hat \lambda_1]} &= \sqrt{\frac{1}{B-1} 
  \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)(\lambda_1^{b*} -\bar \lambda_1^*)}
\end{align*}

where

\begin{align*}
  \bar \lambda_0^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_0^{b*} \\
  \bar \lambda_1^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_1^{b*}.
\end{align*}

Additionally, we can calculate an estimate for the biases of $\hat \lambda_0$ and $\hat \lambda_1$ using the expressions below.

\begin{equation} \label{eq:bias}
  \begin{split}
  \widehat {\bias_B [\hat \lambda_0]} &= \hat \lambda_0 - \bar \lambda_0^{*} \\
  \widehat {\bias_B [\hat \lambda_1]} &= \hat \lambda_1 - \bar \lambda_1^{*}
  \end{split}
\end{equation}

An improved bias estimate can also be calculated, but requires more complex notation to properly explain. Let $F$ be the cumulative distribution function that we assume $(\vect z, \vect u)$ is sampled from and let $\theta = (\lambda_0, \lambda_1) = T(F)$ be a functional of $F$. We let $\hat F$ be the empirical distribution function of the observed data $(\vect z, \vect u)$ and $\hat F^{b*}, \; b = 1, \dots, B$ be the empirical distribution functon of the corresponding bootstrap sample $(\vect z^{b*}, \vect u^{b*})$. The statistical function of bias can be written as $R((\vect Z, \vect U), F) = T(\hat F) - T(F)$. The bias estimate in Equation (\ref{eq:bias}) was derived as $\frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat F) \right)$. According to \cite{} an improved bias estimate is

\begin{equation*}
 \hat {R}((\vect Z, \vect U), F) = \frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat \bar F^*) \right)
 = \bar \theta^* - T(\bar F^*)
\end{equation*}

??????????????????????? Should I use widehat over whole R expression?

where

\begin{equation*}
  \bar F^* = \frac{1}{B} \sum_{b=1}^B \hat F^{b*}
\end{equation*}

and $\bar \theta^* = (\bar \lambda_0^*, \bar \lambda_1^*)$.


```{r}
# fast implementation of the EM algorithm that only stores and returns
# the final values of lambda0 and lambda1. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
fast.em.algorithm = function(z, u, lambdas.initial, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # Create variable for storing previous iteration value of lambda0 and lambda1 values for each iteration
  lambda0.prev = lambdas.initial[1]
  lambda1.prev = lambdas.initial[2]
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0 = n / sum(u * z + (1 - u) * (1 / lambda0.prev - z / (exp(lambda0.prev * z) - 1) )
    )
    lambda1 = n / sum((1 - u) * z + u * (1 / lambda1.prev - z / (exp(lambda1.prev * z) - 1) )
    )
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0 - lambda0.prev), abs(lambda1 - lambda1.prev))
    
    # update previous iteration value of lamba0 and lambda1 with newly calculated values
    lambda0.prev = lambda0
    lambda1.prev = lambda1
  }
  # return final estimates for lambda0 and lambda1
  return ( c(lambda0, lambda1))
}
```

```{r}
# function
bootstrap.lambdas = function(z, u, B = 1000, initial.lambda0 = 1, initial.lambda1 = 1, epsilon = 1e-6) {
  # initialize matrix for store bootstrapped lambda0 and lambda1 estimates
  lambdas.bootstrap = matrix(NA, ncol = 2, nrow = B+1)
  # the first row is only used for initializing the algorithm and will later be removed
  lambdas.bootstrap[1,] = c(initial.lambda0, initial.lambda1)
  n = length(z)
  z.bootstrap.all = rep(NA, B*n)
  u.bootstrap.all = rep(NA, B*n)
  for (i in 1:B) {
    drawn.obs = sample(1:n, n, replace=TRUE)
    z.bootstrap = z[drawn.obs]
    u.bootstrap = u[drawn.obs]
    lambdas.bootstrap[i+1,] = fast.em.algorithm(z.bootstrap, u.bootstrap, lambdas.bootstrap[i,], epsilon)
    z.bootstrap.all[(1+n*(i-1)):(n*i)] = z.bootstrap
    u.bootstrap.all[(1+n*(i-1)):(n*i)] = u.bootstrap
    
  }
  # removing the initial values from the lambdas matrix
  lambdas.bootstrap = lambdas.bootstrap[-1,]
  
  lambdas.bootstrap.mean = apply(lambdas.bootstrap, 2, mean)
  lambdas.bootstrap.sd = apply(lambdas.bootstrap, 2, sd)
  
  lambdas.est = fast.em.algorithm(z, u, lambdas.bootstrap.mean, epsilon)
  
  
  print("starting")
  lambdas.est.from.mean.cumulative.dist = fast.em.algorithm(z.bootstrap.all, u.bootstrap.all,
                                                            c(1,1), epsilon)
  
  print(lambdas.est.from.mean.cumulative.dist)
  
  return(list(lambda0.est = lambdas.est[1],
              
              lambda1.est = lambdas.est[2],
              
              lambda0.bootstrap.mean = lambdas.bootstrap.mean[1],
              
              lambda1.bootstrap.mean = lambdas.bootstrap.mean[2],
              
              lambda0.bias.est = lambdas.bootstrap.mean[1] - lambdas.est[1],
              
              lambda1.bias.est = lambdas.bootstrap.mean[2] - lambdas.est[2],
              
              lambda0.bias.est.improved = lambdas.bootstrap.mean[1] - lambdas.est.from.mean.cumulative.dist[1],
              
              lambda1.bias.est.improved = lambdas.bootstrap.mean[2] - lambdas.est.from.mean.cumulative.dist[2],
              
              lambda0.bootstrap.sd = lambdas.bootstrap.sd[1],
              
              lambda1.bootstrap.sd = lambdas.bootstrap.sd[2],
              
              corr.lambda0.lambda1 = corr(lambdas.bootstrap)
              ))
}

bootstrap.lambdas(z, u, B = 2000, epsilon = 1e-6)
```

