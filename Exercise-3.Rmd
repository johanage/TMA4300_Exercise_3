---
title: "Excercise 3"
author:
- Johan Fredrik Agerup
- Arne Rustad
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
subtitle: TMA4300 Computer intensive statistical methods Spring 2021
header-includes:
- \usepackage{subfig}
- \usepackage[ruled,vlined]{algorithm2e}
- \usepackage{placeins}
- \SetKw{KwBy}{by}
- \SetKwProg{Init}{init}{}{}
- \SetKw{length}{length}
- \SetKw{KwMean}{mean}
- \SetKw{KwSum}{sum}
- \SetKw{KwSd}{sd}
- \SetKw{KwCorr}{corr}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\sd}{\textrm{SD}}
\newcommand{\bias}{\textrm{Bias}}


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(boot)
library(latex2exp)
library(coda)
library(dplyr)
library(tidyverse)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

In this problem we are going to analyze the non-Gaussian time-series data in `data3A$x`. The data contains a sequence of length $T = 100$. The target is to compare two different estimators. A plot of the data is given in Figure \ref{fig:plot_OG}.

```{r plot_OG, fig.height=3, fig.width=6, fig.cap=" \\label{fig:plot_OG} Plot of the non-Gaussian time series."}
source("probAdata.R")
df_OG_data = data.frame(t = seq(1, 100), Value = data3A$x)
ggplot(data = df_OG_data, aes(x = t, y = Value)) + geom_point()
```

To model the time series data we consider an AR(2) model:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t,
\end{equation}

where $e_t$ are iid with zero mean and constant variance. We are going to use the the least sum of squared residuals (LS) and least sum of absolute residuals (LA) to compute two different estimators for the parameters of interest $\vect \beta = [\beta_1, \beta_2]$. The estimators $\hat {\vect \beta}_{LS}$ and $\hat {\vect \beta}_{LA}$ are obtained by respectively minimising the loss functions $Q_{LS}$ and $Q_{LA}$ with respect to $\vect{\beta}$. The loss functions $Q_{LS}$ and $Q_{LA}$ are defined as

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2\\
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|.
\end{align}

The minimisers $\hat {\vect{\beta}}_{LS}$ and $\hat {\vect{\beta}}_{LA}$ are calculated by the given function \textbf{ARp.beta.est}. We then define the estimated residuals to be:

\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t -\hat{\beta}_1x_{t-1} -\hat{\beta}_2 x_{t-2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. The re-cented residuals $\hat{\epsilon}_t$ can be directly obtained from given function \textbf{ARp.resid} for both methods LS and LA.

```{r}
# import given help functions
source("probAhelp.R", local = knitr::knit_global())
#compute LA and LS estimators for original time series
minimisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)
```


For our given time series data we get $\hat {\vect \beta}_{LS} = [\hat \beta_{1,LS}, \hat \beta_{2,LS}] = [`r round(minimisers_ar2$LS[1], 3)`, `r round(minimisers_ar2$LS[2], 3)`]$ and $\hat {\vect \beta}_{LA} = [\hat \beta_{1,LA}, \hat \beta_{2,LA}] = [`r round(minimisers_ar2$LA[1], 3)`, `r round(minimisers_ar2$LA[2], 3)`]$.


## 1)

In this task we use residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators. In order to create new bootstrapped time series we pick a random consecutive pair from the original data to initialize each new bootstrap time series. In this case where $\mathbf{x} = \{x_1, \dots, x_{100} \}$ we pick randomly from all possible pairs

\begin{equation}\label{all_possible_consecutive_pairs}
\begin{pmatrix}
x_1 & x_2 & x_3 & \dots & x_{99}\\
x_2 & x_3 & x_4 & \dots & x_{100}.
\end{pmatrix}
\end{equation}

The rest of each bootstrapped time series is then computed iteratively by the AR(2) scheme in Equation (\ref{AR2}). Here the new residuals are sampled with replacement from the residuals computed for the original time series. This is done for both methods LA and LS. We do this using the given function \textbf{ARp.filter}.

```{r}
# compute LA and LS resiudals for original time series
eps_residuals_LS = ARp.resid(x = data3A$x, beta = minimisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minimisers_ar2$LA)

# function for picking x1 and x2 values
#by picking a random consecutive subsequence from the data
sample_consecutive = function(x){
  "Input:
  - original time series
  Output:
  - a consecutive pair of data points from the original data series"
  idx = sample(seq(1,length(x)-1), size = 1, replace = FALSE)
  return(x[idx:(idx+1)])
}
```

In Figure \ref{LS_resampling} and Figure \ref{LA_resampling} we plot a bootstrapped time series realization for respectively the LS and LA model. They both appear quite reasonable. The reason they are so similar is that they are both computed using the same seed and consequently the same time steps are drawn for the residual resampling step. Note that although the drawn time steps are the same, the drawn residuals still differ a bit due to being estimated from respectively $\hat {\vect \beta}_{LS}$ and $\hat {\vect \beta}_{LA}$.

```{r, fig.height=3, fig.width=6, fig.cap=" \\label{LS_resampling} Plot of a bootstrapped time series realization using the LS model."}
# do a resampling using the LS residuals and plot the samples
set.seed(1)
x1x2_sample1 = sample_consecutive(data3A$x)
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minimisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=length(eps_residuals_LS), replace=TRUE))
df_samples_LS = data.frame(t = seq(1,100), Value = calc_x_LS)
ggplot(data = df_samples_LS) + geom_point(aes(x = t, y = Value)) +
  
  ggtitle("Bootstrapped realization for LS model")
```

```{r, fig.height=3, fig.width=6, fig.cap=" \\label{LA_resampling} Plot of a bootstrapped time series realization using the LA model."}
# do a resampling using the LA residuals and plot the samples
set.seed(1)
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minimisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=length(eps_residuals_LA), replace=TRUE))
df_samples_LA = data.frame(t = seq(1,100), Value = calc_x_LA)
ggplot(data = df_samples_LA) + geom_point(aes(x = t, y = Value)) +
  
  ggtitle("Bootstrapped realization for LA model")
```


\FloatBarrier

To evaluate the performance of the two estimators $\hat {\vect{\beta}}_{LS}$ and $\hat {\vect{\beta}}_{LA}$ we choose to look at the estimates for the variance and the bias of each estimator. We will use our bootstrapped time series to do this. We denote $s(\vect{x})$ as the $\vect \beta$ values that minimises a general loss function $Q(\vect{x})$. The following algorithm for calculating the variance and bias for the estimators in the residual bootstrapping is then given by

1. Generate $B$ bootstrap samples $\vect{x}^{1*}, \dots, \vect{x}^{B*}$ for both LS and LA method using the estimators $\hat{\vect{\beta}}_{LS}$ and $\hat{\vect{\beta}}_{LA}$ along with residual resampling from original time series $\vect x$.
2. Evaluate the corresponding parameter estimates for both methods:
\begin{equation*}
\hat{\vect \beta}^{b*} = [\beta_1^{b*}, \beta_2^{b*}] = s(\vect{x}^{b*}), \qquad b = 1,2, \dots, B.
\end{equation*}
3. Estimate the variance of $\hat \beta_1$ and $\hat \beta_2$ for both methods by:

\begin{align*}
\widehat{\mathrm{Var}}[\hat \beta_1] &= \frac{ \sum_{b=1}^B \left( \hat{\beta}_1^{*b} - \bar{\beta}_1^{*} \right)^2 }{B-1} \\
\widehat{\mathrm{Var}}[\hat \beta_2] &= \frac{ \sum_{b=1}^B \left( \hat{\beta}_2^{*b} - \bar{\beta}_2^{*} \right)^2 }{B-1} \\
\end{align*}

where:

\begin{equation*}
\bar{\beta}_i^{*} \equiv \frac{1}{B}\sum_{b=1}^B \hat{\beta}_i^{b*}, \qquad \textrm{for } i = 1,2
\end{equation*}

4. Calculate bias of $\hat \beta_1$ and $\hat \beta_2$ for both methods by:
\begin{align*}
\widehat{\mathrm{Bias}}(\hat{\beta}_1) =  \bar{\beta}_1^{*} - \hat \beta_1 \\
\widehat{\mathrm{Bias}}(\hat{\beta}_2) =  \bar{\beta}_2^{*} - \hat \beta_2
\end{align*}

where $\bar{\beta_i}^{*}$ represents the bootstrap mean of the estimator $\beta_i$ for the LS or LA method and $\hat \beta_i$ is the LS or LA estimator for $\beta_i$ calculated from the original time series.


```{r}
# bootstrap resampling algorithm
set.seed(2021)
residual.bootstrap.resampling = function(x, p, B){
  "
  Inputs:
  - x = original time series data, type = vector
  - p is the degree of the AR(p) model you want to use, type = int
  - B is the number of bootstrap samples you want, type = int
  Outputs:
  - betas_LS_est = beta estimators using the LS method, type = matrix, nrow = B, ncol = 2
  - betas_LA_est = beta estimators using the LA method, type = matrix, nrow = B, ncol = 2
  - var_beta_LS = estimated variance using the estimators from the LS method, type = vector
  - var_beta_LA = estimated variance using the estimators from the LA method, type = vector
  - bias_beta_LS = estimated bias using the estimators from the LS method, type = vector
  - bias_beta_LA = estimated bias using the estimators from the LA method, type = vector
  - x101_LS = simulated value of x101 using the data from the LS method, type = vector
  (for use in task A2)
  - x101_LA = simulated value of x101 using the data from the LA method, type = vector
  (for use in task A2)
  "
  # initialize vectors for storing beta estimates for LS and LA method
  beta_est_LS = matrix(NA, nrow = B, ncol = 2)
  beta_est_LA = matrix(NA, nrow = B, ncol = 2)
  
  #calculating the minimisers for the original data
  beta_vec_OG = ARp.beta.est(x = x, p = p)
  
  #calculating the recentered residuals for the original data for LS and LA method
  eps_LS = ARp.resid(x = x, beta = beta_vec_OG$LS)
  eps_LA = ARp.resid(x = x, beta = beta_vec_OG$LA)
  
  #initializing vectors for storing the x101 samples for LS and LA method
  x101_LA = rep(NA, B)
  x101_LS = rep(NA, B)
  
  for(b in 1:B){
    #sample pair from original time series data
    x_pair = sample_consecutive(x = x)
    
    #sampling full bootstrap time series by initializing with drawn consecutive pair and
    #resampling with replacement from the residuals for original time series and
    #corresponding model
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vec_OG$LS, 
                         e = sample(eps_LS, size=length(eps_LS), replace=TRUE))
    x_LA = ARp.filter(x0 = x_pair, 
                     beta = beta_vec_OG$LA, 
                     e = sample(eps_LA, size=length(eps_LA), replace=TRUE))
    
    #calculate estimators based upon bootstrapped time series and adding to list of beta estimates
    beta_est_LS[b,] = ARp.beta.est(x = x_LS, p = p)$LS
    beta_est_LA[b,] = ARp.beta.est(x = x_LA, p = p)$LA
    
    ############# Begin step for task A2
    # using bootstrap sample to calc residuals with estimated betas
    e_est_LS = ARp.resid(x = x_LS, beta = beta_est_LS[b,])
    e_est_LA = ARp.resid(x = x_LA, beta = beta_est_LA[b,])
    
    #creating the x101 sample by sampling from the bootstrap residuals and using the estimated betas
    #along with x100 and x99 from original time series

    e_101_LS = sample(e_est_LS, size = 1)
    x101_LS[b] = beta_est_LS[b, 2]*x[length(x)] + beta_est_LS[b, 1]*x[length(x)-1] + e_101_LS
    
    e_101_LA = sample(e_est_LA, size = 1)
    x101_LA[b] = beta_est_LA[b, 2]*x[length(x)] + beta_est_LA[b, 1]*x[length(x)-1] + e_101_LA
    
    ########### End step for task A2
  }
  #calculate the estimates of the variance of the estimators
  var_beta_LS = apply(beta_est_LS, 2, var)
  var_beta_LA = apply(beta_est_LA, 2, var)
  #calculate the estimates of the bias of the estimators
  bias_beta_LS = apply(beta_est_LS, 2, mean) - beta_vec_OG$LS
  bias_beta_LA = apply(beta_est_LA, 2, mean) - beta_vec_OG$LA
  return(list(betas_LS_est = beta_est_LS,
              
              betas_LA_est = beta_est_LA,
              
              var_beta_LS = var_beta_LS,
              
              var_beta_LA = var_beta_LA,
              
              bias_beta_LS = bias_beta_LS,
              
              bias_beta_LA = bias_beta_LA,
              
              x101_LS = x101_LS,
              
              x101_LA = x101_LA))
}

#estimating the variance and bias of the two estimators
set.seed(1)
result.res.bootstrap = residual.bootstrap.resampling(x = data3A$x, p = 2, B = 10000)
```


\begin{table}[!h]
\centering
\begin{tabular}{| l | r | r |}
\hline
 & LS estimator & LA estimator\\
\hline
$\widehat {\Var} [\hat \beta_1]$ & $`r signif(result.res.bootstrap$var_beta_LS[1],3)`$ & $`r signif(result.res.bootstrap$var_beta_LA[1],3)`$ \\
$\widehat {\Var} [\hat \beta_2]$ & $`r signif(result.res.bootstrap$var_beta_LS[2],3)`$ & $`r signif(result.res.bootstrap$var_beta_LA[2], 3)`$ \\
$\widehat {\bias} [\hat \beta_1]$ & $`r signif(result.res.bootstrap$bias_beta_LS[1],3)`$ & $`r signif(result.res.bootstrap$bias_beta_LA[1], 3)`$ \\
$\widehat {\bias} [\hat \beta_2]$ & $`r signif(result.res.bootstrap$bias_beta_LS[2],3)`$ & $`r signif(result.res.bootstrap$bias_beta_LA[2], 3)`$ \\
\hline
\end{tabular}
\caption{Table containing the variance and bias estimates from the residual resampling bootstrap comparing the LS and LA model.}
\label{table:residual_bootstrap}
\end{table}

The LS estimator is optimal for Gaussian AR(p) processes, however, from Table \ref{table:residual_bootstrap} we see that for this problem the LA estimator appear to be superior. The LA estimator has both lower variance by a factor of 10 and lower bias by a factor of around 5 for both $\hat \beta_1$ and $\hat \beta_2$. Therefore, we prefere the LA estimates for this problem.


## 2)

In this task we want to compute a prediction interval for $x_{101}$. To do this we slightly altered the function \textbf{residual.bootstrap.resampling} in task A1. For each bootstrapped time series $\vect x^{b*}$ and parameter estimates $\hat {\vect \beta}^{b*}$ we 

1. Estimate the corresponding residual distribution
2. Draw one residual $\epsilon_{101}^{b*}$ from the estimated residual distribution
2. Use the residual $\epsilon_{101}^{b*}$ combined with bootstrap parameter estimates $\hat {\vect \beta}^{b*}$ to simulate a value $x_{101}^{b*}$. This is done using the AR(2) scheme in Equation (\ref{AR2}) and $x_{100}$ and $x_{99}$ from the original time series.

Let $\vect x_{101}^{*} = [x_{101}^{1*}, x_{101}^{2*}, \dots, x_{101}^{B*}]$. We find the limits of the $95\%$ prediction interval for $x_{101}$ as the the 0.025 quantile of $\vect x_{101}^{*}$ and the 0.975 quantile of $\vect x_{101}^{*}$.

This is done for both the LS and LA method.


```{r plot_x101_LS, fig.cap=" \\label{plot_x101_LS} Plot of the $x_{101}$'s simulated by LS. The vertical lines represents the $95\\%$ prediction interval."}
#computing prediction interval for LS
q_x101_LS = stats::quantile(x = result.res.bootstrap$x101_LS, probs = c(0.025, 0.975))

#computing prediction interval for LS
q_x101_LA = stats::quantile(x = result.res.bootstrap$x101_LA, probs = c(0.025, 0.975))
```

For the LS method the $95\%$ prediction interval for $x_{101}$ is $[`r round(q_x101_LS[1],2)`, `r round(q_x101_LS[2],2)`]$, while for the LA method the interval is $[`r round(q_x101_LA[1],2)`, `r round(q_x101_LA[2],2)`]$. We observe that the LA method prediction interval is longer than the corresponding LS method prediction interval. This might just be due to randomness. In Figure \ref{plot_x101} we plot the distribution of the simulated $x_{101}$ samples for both the LS and the LA method.

```{r, fig.width = 6, fig.cap=" \\label{plot_x101} Histogram of the $x_{101}$'s simulated by the LA and the LS method. The vertical lines represents the $95\\%$ prediction interval."}
# generating df and plotting the histogram for x101 samples
B = length(result.res.bootstrap$x101_LS)
df.x101 = data.frame(x101 = c(result.res.bootstrap$x101_LS, result.res.bootstrap$x101_LA), 
                     q.lower = c(rep(q_x101_LS[1],B), rep(q_x101_LA[1], B)),
                     q.upper = c(rep(q_x101_LS[2],B), rep(q_x101_LA[2], B)),
                     method = factor(c(rep("LS", B), rep("LA", B)), levels = c("LS", "LA"))
)

ggplot(df.x101) + geom_histogram(aes(x = x101), bins = 50) + geom_vline(aes(xintercept = q.lower)) + geom_vline(aes(xintercept = q.upper)) + facet_wrap(~method) +
  ylab("Count") + xlab(TeX("$x_{101}$")) + ggtitle(TeX("Simulated samples for $x_{101}$ for both methods"))
```

\FloatBarrier



# Problem B Permutation Test

In this task we look at data that contain measurements of the concentration of bilirubin (mg/dL) in blood samples taken from three young men. Bilirubin is a breakdown product of haemoglobin, which is a principal component of red blood cells. If the liver has suffered degeneration, if the decomposition of haemoglobin is elevated, or if the gall bladder has been  destroyed, large amounts of bilirubin can accumulate in the blood, leading to jaundice. We will use the F-statistic to perform a Permutation test. For testing a hypothesis $\vect \beta = \vect \beta_0$ the F-statistic is defined

\begin{equation*}
F = \frac{\left( RSS(\beta_0) - RSS(\beta_1)\right) / (p_1 - p_0)}{RSS(\beta_1) / (n - p_1)}
\end{equation*}

where $RSS(\beta_0)$ is the residual sum of squares for the model under the null hypothesis and $RSS(\beta_1)$ is the residual sum of squares for the model under the alternative hypothesis. Here $n$ is the number of observations and $p_0$ and $p_1$ are the model degrees of freedom under respectively the null hypothesis and the alternative hypothesis.


```{r}
# Fetching the data
df.bilirubin <- read.table("bilirubin.txt", header=T)
```


## 1)

To begin we fit a boxplot of the logarithms of concentrations for each individual.


```{r, fig.width = 6, fig.cap="\\label{fig:boxplot} Boxplot for the logarithms of concentrations for each individual. The unit for the concentrations is $\\textrm{mg/dL}$."}
# Plotting a boxplot for each person
ggplot(df.bilirubin, aes(x = pers, y = log(meas))) + 
  geom_boxplot() + 
  ggtitle("Boxplot of logarithm of concentration") + 
  ylab("log(measurement)") + 
  xlab("Person")
```

From Figure \ref{fig:boxplot} we observe that the measurements for person 3 differ quite drastically from the two other persons, while the measurements for person 1 and person 2 have approximately equal median. The measurements for person 1 does, however, appear to vary more than the measurements for person 2.

Next we fit the linear regression model

\begin{equation} \label{eq:model_B1}
\ln(Y_{i,j}) = \beta_i + \epsilon_{i,j}, \qquad \textrm{with } i = 1,2,3 \textrm{ and } j = 1, \dots, n_i
\end{equation}

where $n_1 = 11$, $n_2 = 10$ and $n_3 = 8$ and $\epsilon_{i,j} \sim \mathcal{N}(0, \sigma^2)$. 

```{r}
model.bilirubin <- lm(log(meas)~pers, data=df.bilirubin)
summary.model.bilurbin = summary.lm(model.bilirubin)
```

Next we use the F-test to test the hypothesis $\beta_1 = \beta_2 = \beta_3$.

\begin{align*}
&\textrm{H$_0$: } \beta_1 = \beta_2 = \beta_3 \\
&\textrm{H$_1$: At least one $\beta_i$ is different from the other $\beta_i$s}
\end{align*}

We use significance level $\alpha = 0.05$ for the hypothesis test.

```{r}
Fval = summary.model.bilurbin$fstatistic
p.value = pf(Fval[names(Fval) == "value"], 
             Fval[names(Fval) == "numdf"], 
             Fval[names(Fval) == "dendf"],
             lower.tail = FALSE)
```

The F-statistic is $`r round(Fval[names(Fval) == "value"], 3)`$ with $(`r Fval[names(Fval) == "numdf"]`, `r Fval[names(Fval) == "dendf"]`)$ degrees of freedom. The corresponding p-value is $`r round(p.value,4)`$. As this p-value is lower than our chosen significance level $\alpha = 0.05$ we reject the null hypothesis and conclude that at least one $\beta_i$ is different from the other.


```{r, echo=FALSE}
model.bilirubin <- lm(log(meas)~pers-1, data=df.bilirubin)
summary.model.bilurbin = summary.lm(model.bilirubin)
coef = summary.model.bilurbin$coefficients[,1]
```

Hence we get that 
\begin{align*}
\hat \beta_1 &= `r round(coef[1],3)` \\
\hat \beta_2 &= `r round(coef[2],3)` \\
\hat \beta_3 &= `r round(coef[3],3)` \\
\end{align*}



## 2)

Next we write a function \textbf{permTest} which generates a permutation of the data between the three individuals, consequently fits the model given in (\ref{eq:model_B1}) and finally returns the value of the F-statistic for testing $\beta_1 = \beta_2 = \beta_3$.

```{r}
# Function that performs one iteration of the Permutation test
permTest = function(df) {
  # permute the data
  n = nrow(df)
  obs.shuffled = sample(1:n, n, replace = FALSE)
  df.shuffled = data.frame(meas = df$meas[obs.shuffled], pers =df$pers)
  # fit the model on the shuffled data
  model.shuffled <- lm(log(meas)~pers, data=df.shuffled)
  # calculate the F-statistic and return it
  fstatistic = summary.lm(model.shuffled)$fstatistic[1]
  return(fstatistic)
}
```

## 3)

In this task we perform a permutation test using the function \textbf{permTest} to generate a sample of size 999 for the F-statistic.


```{r, fig.width = 6}
# Function for performing permutation test
permutation.test = function(df, Fval, B = 999, plot.hist = TRUE) {
  "
  Input:
  - df: data frame for which the permutation test should be performed
  - Fval: F-statistic calculated for the model fitted to the original dataset
  - B: number of generated bootstrap samples
  - plot.hist: Boolean determining whether a histogram of the estimated density function
  of the F-statistic under the null hypothesis should be plotted
  Output:
  - A list containing
    -- p.value: The estimated p-value for the permuation test
    -- fstatistic.vec
  "
  # Initialize vector for storing F-statistics
  fstatistic.vec = rep(NA, B)
  # Calculcate F-statistics from permutated datasets
  for (i in 1:B) fstatistic.vec[i] = permTest(df)
  # if hist == TRUE plot a histogram for the estimated density function of the F-statistic
  if (plot.hist) {
    fig.hist = ggplot(data.frame(fstatistic = fstatistic.vec), 
                      aes(x = fstatistic, y = ..density..)) +
      
      geom_histogram(bins = 50) + xlab("F-statistic") + ylab("Density") +
      
      ggtitle("Histogram of the F-statistic under the null hypothesis") +
      
      geom_vline(aes(xintercept=Fval, 
                     col = "F-statistic calculated from original dataset"), 
                 data = data.frame(Fval = Fval[1]), linetype = "dashed") +
      
      theme(legend.position = "bottom") + guides(col = guide_legend("Vertical line"))
      
    print(fig.hist)
  }
  # Calculate the p-value from the permutation test using the histogram as
  # an approximation for the distribution of the F-statistic
  p.value = sum(fstatistic.vec >= Fval) / B
  # Return the p-value and the simulated F-statistics under the null hypothesis
  return(list(p.value = p.value, fstatistic.vec = fstatistic.vec))
}
```


```{r, fig.width = 6, fig.cap="\\label{fig:histogram} Histogram of estimated F-statistics during the Permutation test. The histogram approximates the density function of the F-statistic under the null hypothesis. The vertical, red, dashed line is the original F-statistic value. "}
set.seed(1)
p.value.permutation.test = permutation.test(df.bilirubin, Fval, B = 999)$p.value
```

In Figure \ref{fig:histogram} we plot a histogram of the F-statistics computed during the Permutation test, which is an approximation for the density function of the F-statistic under the null hypothesis. From the plot it appears that the F-statistic computed from the original data is a part of the upper tail of the distribution function for the F-statistic under the null hypothesis.

The permutation test gives a p-value of $`r round(p.value.permutation.test,4)`$ for the hypothesis test. We observe that this value is a bit larger than the p-value we computed in Task C1 which was $`r round(p.value,4)`$. The p-value from the permutation test is larger than our chosen significance level $\alpha = 0.05$ and hence we do not have strong enough evidence to reject the null hypothesis, which means that:



```{r, echo=FALSE}
model.bilirubin.null.hypothesis <- lm(log(meas)~1, data=df.bilirubin)
summary.model.bilurbin.null.hypothesis = summary.lm(model.bilirubin.null.hypothesis)
coef.null.hypothesis = summary.model.bilurbin.null.hypothesis$coefficients[,1]
```


\begin{align*}
\hat \beta_1 = \hat \beta_2 = \hat \beta_3 = `r round(coef.null.hypothesis,3)`\\
\end{align*}

Note that in this case the second p-value  is larger than the significance level and the first is smaller. Hence the chosen significance level gives rise to different conclusions of the hypethesis test. However, we have run the same permutation test multiple times and this was not always the case. This is due to randomness of the permutation test. 

# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i), \; i = 1, \dots, n$, we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f_{\vect X, \vect Y}(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f_{\vect X}(\vect x | \lambda_0) f_{\vect Y}(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f_{X_i}(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f_{Y_i}(y_i | \lambda_1) \right ] \\
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{equation*}
\begin{split}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \ln \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{split}
\end{equation*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $(\lambda_0^{(t)}, \lambda_1^{(t)})$.

\begin{equation} \label{eq:em_formula_incomplete}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \ln\left( f_{\vect X, \vect Y}(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E \left[ x_i | z_i, u_i, \lambda_0^{(t)}\right]  - \lambda_1 \sum_{i=1}^n \E \left[ y_i | z_i, u_i, \lambda_1^{(t)} \right]
\end{split}
\end{equation}

For the case $u_i = 1$ then we have

\begin{align*}
\E \left[x_i | z_i, u_i = 1, \lambda_0^{(t)} \right] &= u_i
\end{align*}

and

\begin{align*}
\E \left[y_i | z_i, u_i = 1, \lambda_1^{(t)} \right]
&= \int_0^{z_i} y_i P \left( y_i | y_i \leq z_i, \lambda_1^{(t)}\right) dy_i \\
&=\int_0^{z_i} y_i \frac{P \left( y_i \cap y_i \leq z_i | \lambda_1^{(t)} \right) }
{P \left( y_i \leq z_i | \lambda_1^{(t)} \right) } dy_i \\
&= \int_0^{z_i} y_i \frac{f(y_i | \lambda_1^{(t)}) I(y_i \leq z_i)}{ \int_0^{z_i} f(y_i^* | \lambda_1^{(t)}) dy_i^*} dy_i \\
&= \frac{1}{ \int_0^{z_i} \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i^* } dy_i^*}
\int_0^{z_i} y_i \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i} dy_i \\
&= \frac{1}{1 - e^{- \lambda_1^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_1^{(t)} z_i} - \frac{1}{\lambda_1^{(t)}} e^{- \lambda_1^{(t)} z_i} + \frac{1}{\lambda_1^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_1^{(t)} z_i}}{1 - e^{- \lambda_1^{(t)} z_i}} + \frac{1}{\lambda_1^{(t)}} \\
&= \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
\end{align*}

Correspondlingly, we have for the $u_i = 0$ that

\begin{align*}
\E \left[ x_i | z_i, u_i = 0, \lambda_0^{(t)} \right]
&= \int_0^{z_i} x_i P \left( x_i | x_i \leq z_i, \lambda_0^{(t)}\right) dx_i \\
&=\int_0^{z_i} x_i \frac{P \left( x_i \cap x_i \leq z_i | \lambda_0^{(t)} \right) }
{P \left( x_i \leq z_i | \lambda_0^{(t)} \right) } dx_i \\
&= \int_0^{z_i} x_i \frac{f(x_i | \lambda_0^{(t)})}{ \int_0^{z_i} f(x_i^* | \lambda_0^{(t)}) dx_i^*} dx_i \\
&= \frac{1}{ \int_0^{z_i} \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i^* } dx_i^*}
\int_0^{z_i} x_i \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i} dx_i \\
&= \frac{1}{1 - e^{- \lambda_0^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_0^{(t)} z_i} - \frac{1}{\lambda_0^{(t)}} e^{- \lambda_0^{(t)} z_i} + \frac{1}{\lambda_0^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_0^{(t)} z_i}}{1 - e^{- \lambda_0^{(t)} z_i}} + \frac{1}{\lambda_0^{(t)}} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{align*}

and

\begin{align*}
\E \left[ y_i | z_i, u_i = 0, \lambda_1^{(t)} \right] &= u_i \\
\end{align*}

Inserting this result into Equation (\ref{eq:em_formula_incomplete}) we get

\begin{equation} \label{eq:em_formula}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
=& \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
\big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
=& n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) \\
&- \lambda_0 \sum_{i=1}^n 
\left [ u_i z_i 
+ (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1} \right)
\right] \\
&- \lambda_1 \sum_{i=1}^n \left [
(1 - u_i) z_i + 
u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1} \right)
\right ]
\end{split}
\end{equation}

## 2)

Let $Q(\lambda_0, \lambda_1)$ be defined

\begin{equation*}
Q(\lambda_0, \lambda_1)
= Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)})
= \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
\end{equation*}

We want to use the Expectation-Maximization (EM) algorithm as defined in Algorithm \ref{em_algorithm} to find the maximum likelihood estimates of $(\lambda_0, \lambda_1)$ iteratively. We denote these estimates $(\hat \lambda_0, \hat \lambda_1)$.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Observed data $(\vect z, \vect u)$ and starting values $\lambda_0^{(0)}, \lambda_1^{(0)} $}
 \KwOut {Maximum likelihood estimates for $(\lambda_0, \lambda_1)$.}
 $t \gets 0$\;
 \While{not converged}{
  $Q(\lambda_0, \lambda_1) = \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]$ \;
  $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)}) = \argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1) $\;
    $t \gets t + 1$ \;
 }
 \Return{$(\hat \lambda_0, \hat \lambda_1) = (\lambda_0^{(t)}, \lambda_1^{(t)})$}
 \caption{EM algorithm to maximize $l(\lambda_0, \lambda_1 |  \vect z, \vect u)$}
 \label{em_algorithm}
\end{algorithm}

In Task C1 we found an expression for $Q(\lambda_0, \lambda_1)$. To be able to implement Algorithm \ref{em_algorithm} we need to find an expression for $\argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1)$. To maximize $Q(\lambda_0, \lambda_1)$ with respect to $(\lambda_0, \lambda_1)$ we set the partial derivatives equal to zero and solve for each of the parameters.

\begin{align}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_0} - \sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ] = 0 \nonumber \\
  \lambda_0 = \frac{n}
  {\sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda0}
\end{align}

\begin{align}
  \frac{\partial}{\partial \lambda_1} Q(\lambda_1, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_1} - \sum_{i=1}^n \left[ (1 - u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ] = 0
  \nonumber \\
  \lambda_1 = \frac{n}
  {\sum_{i=1}^n \left[ (1-u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda1}
\end{align}

To ensure we have found a maximum we also check the second derivatives

\begin{align*}
\frac{\partial^2}{\partial \lambda_0^2} &= \frac{-n}{\lambda_0^2} < 0\\
  \frac{\partial^2}{\partial \lambda_1^2} &= \frac{-n}{\lambda_1^2} < 0
\end{align*}

As the second derivatives are stricly negative for all $\lambda_0$ and $\lambda_1$ we know that we have found the values that maximizes $Q(\lambda_0, \lambda_1)$. From Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) we have a recursive expression for the maximum likelihood estimates of $(\lambda_0, \lambda_1)$.

Below we implement the EM-algorithm for this specific case and find the maximum likelihood estimates when the data is as specified in the files \textbf{z.txt} and \textbf{u.txt} available from the course home page. As stopping criteria we use that the maximum absolute value parameter change from one iteration to the next must be smaller than some chosen $\epsilon$, that is 
$\max \left \{ \left|\lambda_0^{(t+1)} - \lambda_0^{(t)}\right|, \left|\lambda_1^{(t+1)} - \lambda_1^{(t)}\right| \right \} < \epsilon$.

```{r}
# implementation of the EM algorithm. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
em.algorithm = function(z, u, lambda0, lambda1, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # create vectors for storing lambda0 and lambda1 values for each iteration
  lambda0.vec = c(lambda0)
  lambda1.vec = c(lambda1)
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  t = 1
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0.vec = c(lambda0.vec, 
                       n / sum(u * z + (1 - u) * (1 / lambda0.vec[t] - 
                                                    z / (exp(lambda0.vec[t] * z) - 1) ) )
    )
    lambda1.vec = c(lambda1.vec,
                    n / sum((1 - u) * z + u * (1 / lambda1.vec[t] - 
                                                 z / (exp(lambda1.vec[t] * z) - 1) ) )
    )
    t = t + 1
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0.vec[t] - lambda0.vec[t-1]), abs(lambda1.vec[t] - lambda1.vec[t-1]))
  }
  # get the number of iterations
  n.iter = t-1
  # create a data frame for storing parameter values for each iteration for easy plotting
  df = data.frame(iter = seq(0,n.iter), lambda0 = lambda0.vec, lambda1 = lambda1.vec)
  # same data frame on long format (also for easy plotting)
  df.long = pivot_longer(df, -iter, names_to = "Parameter")
  # return a list with useful values
  return ( list(lambda0 = lambda0.vec[t],lambda1 = lambda1.vec[t],
                lambda0.vec = lambda0.vec, lambda1.vec = lambda1.vec, n.iter = n.iter,
                df = df, df.long = df.long))
}
```


```{r}
# fetch the data
z = read.csv("z.txt")[,1]
u = read.csv("u.txt")[,1]
# find maximum likelihood estimates
result.em = em.algorithm(z, u, 1, 1)
```

The maximum likelihood estimates of $(\lambda_0, \lambda_1)$ are $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 3)`, `r round(result.em$lambda1, 3)`)$. Next we test the convergence of the algorithm for different initial values.

```{r, fig.width=8, fig.height = 3, fig.cap = "\\label{fig:convergence_plot} Visualization of convergence for each of the parameters $(\\lambda_0,\\lambda_1)$ for three different initial values $(\\lambda_0^{(0)}, \\lambda_1^{(0)}) \\in [(1,1), (7,7), (15, 15)]$. As stopping criteria we use $\\textrm{max} [ |\\lambda_0^{(t+1)} - \\lambda_0^{(t)}|, |\\lambda_1^{(t+1)} - \\lambda_1^{(t)}| ] < 10^{-6}$."}
# test for different initial values
# choose the initial value combinations
initial.lambda0 = c(1, 7, 15)
initial.lambda1 = c(1, 7, 15)

# create a data frame to store all the realizations
df.em.long = data.frame()
for (i in 1:length(initial.lambda0)) {
  result.em.i = em.algorithm(z, u, initial.lambda0[i], initial.lambda1[i])
  df.em.long = rbind(df.em.long, cbind(result.em.i$df.long, "initial.value" = toString(i)))
}
# visialize convergence for different initial values
ggplot(df.em.long, aes(x = iter, y = value, col = initial.value)) + geom_line() +
  
  facet_wrap(~Parameter) + guides(col = guide_legend("Initial values")) +
  
  scale_color_discrete(labels = c("(1,1)", "(7,7)", "(15,15)")) +
  
  ggtitle("Convergence plot of estimates")
```

From Figure \ref{fig:convergence_plot} we see that the algorithm quickly converges for all three initial conditions. The estimate for $\lambda_0$ appear to be almost converged after only a single iteration, while the estimate for $\lambda_1$ seem to need around 5 iterations for the same level of precicion. After 16 iterations all three initial positions were converged according to $\epsilon = 10^{-6}$. An explanation for why the estimate for $\lambda_0$ converges much quicker than the estimate for $\lambda_1$ is that $\hat \lambda_1 = `r round(result.em$lambda1,3)`$ is a lot larger than $\hat \lambda_0 = `r round(result.em$lambda0,3)`$. Consequently, $\hat E[x_i] = \frac{1}{\hat \lambda_0} = `r round(1 / result.em$lambda0,3)` > \hat E[y_i] = \frac{1}{\hat \lambda_1} =`r round(1 / result.em$lambda1,3)`$ and we expect $x_i = z_i$ much more often than $y_i = z_i$. This is also seen from the data where $\sum_{i=1}^n I(u_i = 1) = `r sum(u == 1)`$ and $\sum_{i=1}^n I(u_i = 0) = `r sum(u == 0)`$. Hence we have much more accurate observations for $x_i$ than we have for $y_i$ and therefore it is easier to compute $\hat \lambda_0$ than $\hat \lambda_1$.

## 3)

In this task we use bootstrapping to estimate the standard deviations and the biases of $\hat \lambda_0$ and $\hat \lambda_1$ along with an estimate for $\Corr [\hat \lambda_0, \hat \lambda_1]$. First we define som notation. We will bootstrap $B$ new datasets $(\vect z^{b*}, \vect u^{b*}), \; b = 1, \dots, B$. These datasets will each be used to calculate the two estimators of interest $(\hat \lambda_0^{b*}, \hat \lambda_1^{b*})$ using Algorithm \ref{em_algorithm} on $(\vect z^{b*}, \vect u^{b*})$. From the $B$ pairs of parameter estimates we can estimate the standard deviations of $\hat \lambda_0$ and $\hat \lambda_1$ in addition to the correlation between them.

\begin{align*}
  \widehat {\sd}_{B}[\hat \lambda_0] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)^2} \\
  \widehat {\sd}_{B}[\hat \lambda_1] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_1^{b*} -\bar \lambda_1^*)^2} \\
  \widehat {\Corr}_{B}[\hat \lambda_0, \hat \lambda_1] &= \sqrt{\frac{1}{B-1} 
  \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)(\lambda_1^{b*} -\bar \lambda_1^*)}
\end{align*}

where

\begin{align*}
  \bar \lambda_0^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_0^{b*} \\
  \bar \lambda_1^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_1^{b*}.
\end{align*}

Additionally, we can calculate an estimate for the biases of $\hat \lambda_0$ and $\hat \lambda_1$ using the expressions below.

\begin{equation} \label{eq:bias}
  \begin{split}
  \widehat {\bias}_B [\hat \lambda_0] &= \bar \lambda_0^{*} - \hat \lambda_0  \\
  \widehat {\bias}_B [\hat \lambda_1] &= \bar \lambda_1^{*} - \hat \lambda_1
  \end{split}
\end{equation}

An improved bias estimate can also be calculated, but requires more complex notation to properly explain. Let $F$ be the cumulative distribution function that we assume $(\vect z, \vect u)$ is sampled from and let $\theta = (\lambda_0, \lambda_1) = T(F)$ be a functional of $F$. We let $\hat F$ be the empirical distribution function of the observed data $(\vect z, \vect u)$ and $\hat F^{b*}, \; b = 1, \dots, B$ be the empirical distribution functon of the corresponding bootstrap sample $(\vect z^{b*}, \vect u^{b*})$. The statistical function of bias can be written as $R((\vect Z, \vect U), F) = T(\hat F) - T(F)$. The bias estimate in Equation (\ref{eq:bias}) was derived as $\frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat F) \right)$. According to the book \textit{Computational Statistics, 2nd edition (2013)} by G.H. Givens and J.A. Hoeting an improved bias estimate is

\begin{equation*}
 \widehat {\bias}_{B, \textrm{improved}}[\hat \lambda_0, \hat \lambda_1] = \hat {R}((\vect Z, \vect U), F) = \frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\bar F^*) \right)
 = \bar \theta^* - T(\bar F^*)
\end{equation*}

where

\begin{equation*}
  \bar F^* = \frac{1}{B} \sum_{b=1}^B \hat F^{b*}
\end{equation*}

and $\bar \theta^* = (\bar \lambda_0^*, \bar \lambda_1^*)$.

Let $T_{\lambda_0}(F) = \lambda_0$ and $T_{\lambda_1}(F) = \lambda_1$. Then $T(\bar F^*)$ can be calculated through the EM algorithm with each step being

\begin{align*}
\lambda_0 &= \frac{n} {\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ u_i^{b*} z_i^{b*} 
+ (1 - u_i^{b*}) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_0^{(t)} z_i^{b*} \} - 1} \right) \right]} \\
\lambda_1 &= \frac{n}{\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ (1-u_i^{b*}) z_i^{b*}
+ u_i^{b*} \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_1^{(t)} z_i^{b*} \} - 1} \right) \right ]}
\end{align*}

The derivation for this is not included, but is quite similar to how Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) were derived.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$, observed data $(\vect z, \vect u)$ and the number of bootstrap samples $B$.}
 \KwOut {Estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Calculates two different bias estimates.}
 \Init{}{
 $ \Lambda_B \gets$ Matrix(nrow : $B$, ncol : $2$) \;
 $n \gets \length(\vect z)$ \;
 $\vect z_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 $\vect u_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 }
 \For{$b \gets 1$ \KwTo $B$ \KwBy $1$} {
 $\vect z^{b*} \gets$ bootstrapped version of $\vect z$ of length $n$. \;
 $\vect u^{b*} \gets$ bootstrapped version of $\vect u$ of length $n$. \;
 $(\lambda_0^{b*}, \lambda_1^{b*}) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z^{b*}, \vect u^{b*})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\Lambda_B[b,] \gets (\lambda_0^{b*}, \lambda_1^{b*})$  \;
 $\vect z_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect z^{b*}$ \;
 $\vect u_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect u^{b*}$ \;
 }
 $\widehat {\sd}_B[\hat \lambda_0] \gets \KwSd(\Lambda[,1])$\;
 $\widehat {\sd}_B[\hat \lambda_1] \gets \KwSd(\Lambda[,2])$\;
 $\widehat {\Corr}[\hat \lambda_0, \hat \lambda_1] \gets \KwCorr(\Lambda[,1], \Lambda[,2])$\;
 $\bar \lambda_0^* \gets \KwMean(\Lambda[,1])$\;
 $\bar \lambda_1^* \gets \KwMean(\Lambda[,2])$\;
 $(\hat \lambda_0, \hat \lambda_1) \gets$  maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z, \vect u)$  as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_B[\hat \lambda_0] \gets \bar \lambda_0^* - \hat \lambda_0$\;
 $\widehat {\bias}_B[\hat \lambda_1] \gets \bar \lambda_1^* - \hat \lambda_1$\;
 $(T_{\lambda_0}(\bar F^*), T_{\lambda_1}(\bar F^*)) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z_{B, \textrm{all}}, \vect u_{B, \textrm{all}})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \gets \bar \lambda_0^* - T_{\lambda_0}(\bar F^*)$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \gets \bar \lambda_1^* - T_{\lambda_1}(\bar F^*)$\;
 \SetArgSty{textrm}
 \Return{$\widehat {\mathrm{\sd}}_B[\hat \lambda_0], \widehat {\sd}_B[\hat \lambda_1],  \widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1],\widehat {\bias}_B[\hat \lambda_0], \widehat {\bias}_B[\hat \lambda_1], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$}z
 \caption{Bootstrap algorithm used in task C3}
 \label{bootstrap_algorithm}
\end{algorithm}

In Algorithm \ref{bootstrap_algorithm} it is presented pseudocode for a bootstrap algorithm for calculating estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Below it is implemented in code.


```{r}
# fast implementation of the EM algorithm that only stores and returns
# the final values of lambda0 and lambda1. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
fast.em.algorithm = function(z, u, lambdas.initial, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # Create variable for storing previous iteration value of lambda0 and lambda1 values for each iteration
  lambda0.prev = lambdas.initial[1]
  lambda1.prev = lambdas.initial[2]
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0 = n / sum(u * z + (1 - u) * (1 / lambda0.prev - z / (exp(lambda0.prev * z) - 1) )
    )
    lambda1 = n / sum((1 - u) * z + u * (1 / lambda1.prev - z / (exp(lambda1.prev * z) - 1) )
    )
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0 - lambda0.prev), abs(lambda1 - lambda1.prev))
    
    # update previous iteration value of lamba0 and lambda1 with newly calculated values
    lambda0.prev = lambda0
    lambda1.prev = lambda1
  }
  # return final estimates for lambda0 and lambda1
  return ( c(lambda0, lambda1))
}
```


```{r}
# function for performing what is described in the bootstrap algorithm
bootstrap.lambdas = function(z, u, B = 1000, initial.lambda0 = 1, initial.lambda1 = 1, epsilon = 1e-6) {
  "
  Input:
  - z: observed data as defined earlier (max of x_i and y_i, for i = 1,...,n)
  - u: observed data as defined earlier (1 if x_i >= y_i else 0, for i = 1, ...,n)
  - B: number of bootstrapped samples to use
  - initial.lambda0: initial value for lambda0 for use in em algorithm
  - initial.lambda1 initial value for lambda1 for use in em algorithm
  Output:
  - A list containing
    -- lambda0.est: maximum likelihood estimate for lambda0
    -- lambda1.est: maximum likelihood estimate for lambda1
    -- lambda0.bootstrap.mean: mean of maximum likelihood estimators for lambda0 based
    on bootstrapped data.
    -- lambda1.bootstrap.mean: mean of maximum likelihood estimators for lambda1 based
    on bootstrapped data.
    -- lambda0.bias.est: estimate of bias for lambda0 estimate
    -- lambda1.bias.est: estimate of bias for lambda1 estimate
    -- lambda0.bias.est.improved: improvedestimate of bias for lambda0 estimate
    -- lambda1.bias.est.improved: improvd estimate of bias for lambda1 estimate
    -- corr.lambda0.lambda1: esimate for the correlation between lambda0 and lambda1 estimators 
  "
  
  
  # initialize matrix for store bootstrapped lambda0 and lambda1 estimates
  lambdas.bootstrap = matrix(NA, ncol = 2, nrow = B+1)
  # the first row is only used for initializing the algorithm and will later be removed
  lambdas.bootstrap[1,] = c(initial.lambda0, initial.lambda1)
  n = length(z)
  # initialize vectors for storing all bootstraped data
  z.bootstrap.all = rep(NA, B*n)
  u.bootstrap.all = rep(NA, B*n)
  for (i in 1:B) {
    # draw which observations should be used
    drawn.obs = sample(1:n, n, replace=TRUE)
    # create new bootstrapped datasets
    z.bootstrap = z[drawn.obs]
    u.bootstrap = u[drawn.obs]
    # using the bootstrapped data, calculate the maximum likelihood estimates of
    # lambda0 and lambda1 using em algorithm
    lambdas.bootstrap[i+1,] = fast.em.algorithm(z.bootstrap, u.bootstrap,
                                                lambdas.bootstrap[i,], epsilon)
    # store the bootstrapped datasets
    z.bootstrap.all[(1+n*(i-1)):(n*i)] = z.bootstrap
    u.bootstrap.all[(1+n*(i-1)):(n*i)] = u.bootstrap
    
  }
  # removing the initial values from the lambdas matrix
  lambdas.bootstrap = lambdas.bootstrap[-1,]
  
  # calculating the mean of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.mean = apply(lambdas.bootstrap, 2, mean)
   # calculating the standard deviation of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.sd = apply(lambdas.bootstrap, 2, sd)
  
  # calculate estimates for lambda0 and lambda1 using original observed data
  lambdas.est = fast.em.algorithm(z, u, lambdas.bootstrap.mean, epsilon)
  
    # calculate T(\bar F^*) for use to calculate improved bias estimates
  lambdas.est.from.mean.cumulative.dist = fast.em.algorithm(z.bootstrap.all, u.bootstrap.all,
                                                            c(1,1), epsilon)
  
  return(list(lambda0.est = lambdas.est[1],
              
              lambda1.est = lambdas.est[2],
              
              lambda0.bootstrap.mean = lambdas.bootstrap.mean[1],
              
              lambda1.bootstrap.mean = lambdas.bootstrap.mean[2],
              
              lambda0.bias.est = lambdas.bootstrap.mean[1] - lambdas.est[1],
              
              lambda1.bias.est = lambdas.bootstrap.mean[2] - lambdas.est[2],
              
              lambda0.bias.est.improved = lambdas.bootstrap.mean[1] -
                
                lambdas.est.from.mean.cumulative.dist[1],
              
              lambda1.bias.est.improved = lambdas.bootstrap.mean[2] -
                
                lambdas.est.from.mean.cumulative.dist[2],
              
              lambda0.bootstrap.sd = lambdas.bootstrap.sd[1],
              
              lambda1.bootstrap.sd = lambdas.bootstrap.sd[2],
              
              corr.lambda0.lambda1 = corr(lambdas.bootstrap)
              ))
}

set.seed(1)
result.bootstrap = bootstrap.lambdas(z, u, B = 2000, epsilon = 1e-6)
```

In Table \ref{table:bootstrap_output} it is listed the standard deviations, biases and correlation between $(\hat \lambda_0, \hat \lambda_1)$. All the estimates are calculated using Algorithm \ref{bootstrap_algorithm}. Both of the standard deviation estimates and the correlation estimate appear reasonable. We observe that there is hardly any correlation between the two estimators $\hat \lambda_0$ and $\hat \lambda_1$.

\begin{table}[!h]
\centering
\begin{tabular}{| l | r |}
\hline
Variable & Bootstrap estimate \\
\hline
$\widehat {\mathrm{\sd}}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bootstrap.sd,4)` \\
$\widehat {\sd}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bootstrap.sd,4)` \\
$\widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1]$ & `r round(result.bootstrap$corr.lambda0.lambda1,4)` \\
$\widehat {\bias}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est,4)`\\ 
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est.improved,4)`\\
$\widehat {\bias}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est,4)`\\
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est.improved,4)`\\
\hline
\end{tabular}
\caption{Table containing the output from Algorithm \ref{bootstrap_algorithm} with $(\vect z, \vect u)$ as input observed data, $(\lambda_0^{(0)}, \lambda_1^{(0)}) = (1,1)$ and $B = 2000$.}
\label{table:bootstrap_output}
\end{table}

From Table \ref{table:bootstrap_output} we see that the bias estimates for $\hat \lambda_0$ is very similar for both methods for estimating bias. For $\hat \lambda_1$, however, the improved bias estimate is around twice as large as the other bias estimate. We choose to rely on the improved bias estimate. Consequently, the bias corrected estimates for $\lambda_0$ and $\lambda_1$ are

\begin{align*}
\hat \lambda_0^c &= \hat \lambda_0 - \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \\
&= `r round(result.em$lambda0, 4)` - `r round(result.bootstrap$lambda0.bias.est.improved,4)` \\
&= `r round(result.em$lambda0 - result.bootstrap$lambda0.bias.est.improved,4)` \\
\hat \lambda_1^c &= \hat \lambda_1 - \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \\
&= `r round(result.em$lambda1, 4)` - `r round(result.bootstrap$lambda1.bias.est.improved,4)` \\
&= `r round(result.em$lambda1 - result.bootstrap$lambda1.bias.est.improved,4)`
\end{align*}

As the bias estimates are quite small compared to the estimates of $\lambda_0$ and $\lambda_1$ we prefer the maximum likelihood estimates $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 4)`, `r round(result.em$lambda1, 4)`)$ over the bias corrected estimates $(\hat \lambda_0^c, \hat \lambda_1^c) = (`r round(result.em$lambda0 - result.bootstrap$lambda0.bias.est.improved,4)`, `r round(result.em$lambda1 - result.bootstrap$lambda1.bias.est.improved,4)`)$ since 

\begin{align*}
\Var[\hat \lambda_0^c] = 
\Var[\hat \lambda_0] + \Var \left[ \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \right] +
2 \Cov \left[\hat \lambda_0, \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0]  \right]
\geq \Var[\hat \lambda_0] \\
\Var[\hat \lambda_1^c] 
= \Var[\hat \lambda_1] + \Var \left[ \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \right] +
2 \Cov \left[\hat \lambda_1, \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]  \right]
\geq \Var[\hat \lambda_1]
\end{align*}


## 4)

An analytical formula for $f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1)$ can be derived as

\begin{align*}
f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1) 
&= u_i \cdot P(x_i = z_i | \lambda_0) \cdot P(y_i \leq z_i | \lambda_1) 
+ (1- u_i) P(x_i \leq z_i | \lambda_0) \cdot P(y_i = z_i | \lambda_0) \\
&= u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left[ \int_0^{z_i} \lambda_1 e^{- \lambda_1 y_i} dy_i \right]
+ (1 - u_i) \cdot \left[ \int_0^{z_i} \lambda_0 e^{- \lambda_0 x_i} dx_i\right] \cdot \lambda_1 e^{-\lambda_1 z_i} \\
&= u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left( 1 - e^{- \lambda_1 z_i} \right)
+ (1 - u_i) \cdot \left(1 - \lambda_0 e^{- \lambda_0 z_i} \right) \cdot \lambda_1 e^{-\lambda_1 z_i}
\end{align*}

The likelihood of $(\lambda_0, \lambda_1)$ given the observed data $(\vect z, \vect u)$ is

\begin{align*}
L(\lambda_0, \lambda_1 | \vect z, \vect u) &= f_{\vect Z, \vect U}(\vect z, \vect u | \lambda_0, \lambda_1) \\
&= \prod_{i=1}^n f_{Z_i, U_i}(z_i, u_i | \lambda_0, \lambda_1) \\
&= \prod_{i=1}^n \left[
u_i \cdot \lambda_0 e^{-\lambda_0 z_i} \cdot \left( 1 - e^{- \lambda_1 z_i} \right)
+ (1 - u_i) \cdot \left(1 - \lambda_0 e^{- \lambda_0 z_i} \right) \cdot \lambda_1 e^{-\lambda_1 z_i}
\right]
\end{align*}

Since either $u_i = 1 \land (1-u_i) = 0$ or $u_i = 0 \land (1-u_i) = 1$ we have that the log likelihood of $(\lambda_0, \lambda_1)$ given the observed data $(\vect z, \vect u)$ is

\begin{align*}
l(\lambda_0, \lambda_1 | \vect z, \vect u) &=
\log(L(\lambda_0, \lambda_1 | \vect z, \vect u)) \\
&= \sum_{i=1}^n 
\left( 
u_i \left[ \ln(\lambda_0) - \lambda_0 z_i + \ln(1-e^{- \lambda_1 z_i}) \right]
+ (1 - u_i) \left[ \ln(1-e^{- \lambda_0 z_i}) + \ln(\lambda_1) - \lambda_1 z_i \right]
\right)
\end{align*}

The maximum likelihood estimators (MLE) are $(\hat \lambda_0, \hat \lambda_1) =\argmax_{(\lambda_0, \lambda_1)} L(\lambda_0, \lambda_1 | \vect z, \vect u) = \argmax_{(\lambda_0, \lambda_1)} l(\lambda_0, \lambda_1 | \vect z, \vect u)$. Unfortunately, it is not possible to find analytical formulas for the maximum likelihood estimators. We can, however, find the MLE numerically. An advantage of optimizing the likelihood directly compared to the EM algorithm is that when optimizing the likelihood directly the standard deviations of $\hat \lambda_0, \hat \lambda_1$ and the correlation between them is directly available without bootstrapping.

Let $\hat {\vect \lambda} = [\hat \lambda_0, \hat \lambda_1]^T$. To get an estimate for $\Cov [\hat {\vect \lambda}]$ we use that for independent samples we have that asymptotically as the number of samples goes to infinity the maximum likelihood estimates have the distribution

\begin{equation*}
\hat {\vect \lambda} \sim \mathcal{N}(\vect \lambda, H(\vect \lambda)^{-1})
\end{equation*}

where $H$ is the observed Fisher matrix

\begin{equation*}
H(\vect \lambda) = - \frac{\partial^2}{\partial \vect \lambda \partial \vect \lambda^T} l(\vect \lambda | \vect z, \vect u)
\end{equation*}


Below we optimize the log likelihood function $l(\lambda_0, \lambda_1 | \vect z, \vect u)$ and use that $H(\vect \lambda) \approx H(\hat {\vect \lambda})$ to estimate standard deviations and correlations between the estimators $(\hat \lambda_0, \hat \lambda_1)$.

```{r}
# Create the log likelihood function that shall be optimized with respect to lambda0 and lambda1
log.likelihood = function(lambdas, z, u) {
  return(sum(u * (log(lambdas[1]) - lambdas[1] * z + log(1 - exp(-lambdas[2] * z)))
             + (1-u) * (log(1 - exp(-lambdas[1] * z)) + log(lambdas[2]) - lambdas[2] * z) ) )
}


initial.lambdas = c(1,1) # for initializaing the numerical search
set.seed(1) # set seed for reproducible results
# optimize the log likelihood function with respect to lambda0 and lambda1
# also calculate the Hessian matrix at the optimal values for lambda0 and lambda1
# use fnscale = -1 for maximization instead of minimization
# limit lambda0 and lambda1 to be larger than 0
optim.result = optim(initial.lambdas, 
                     log.likelihood, NULL, z, u, 
                     control = list(fnscale = -1), 
                     hessian=TRUE, lower=c(0,0))
# calculate the mle estimate of the covariance matrix of the estimators for [lambda0, lambda1]
cov.lambdas.mle = solve(- optim.result$hessian)

# calculate standard deviations and correlation between lambda0 and lambda1 estimators
lambda0.sd.mle = sqrt(cov.lambdas.mle[1,1])
lambda1.sd.mle = sqrt(cov.lambdas.mle[2,2])
corr.lambda0.lambda1.mle = cov.lambdas.mle[1,2] / (lambda0.sd.mle * lambda1.sd.mle)
```

\begin{table}[!h]
\centering
\begin{tabular}{| l | r | r|}
\hline
Variable & EM estimate & MLE estimate \\
\hline
$ \hat \lambda_0$ & `r round(result.em$lambda0,4)` & `r round(optim.result$par[1],4)` \\
$ \hat \lambda_1$ & `r round(result.em$lambda1,4)` & `r round(optim.result$par[2],4)` \\
\hline
\end{tabular}
\caption{Comparison of estimates for $\lambda_0$ and $\lambda_1$ computed with Expectation-Maximization (EM) algorithm or direct maximization of likelihood function.}
\label{table:em_mle_comparison}
\end{table}

In Table \ref{table:em_mle_comparison} we compare the estimates from the EM-algorithm against the estimates from direct numerical maximization of the log-likelihood function. The estimates from both methods are very similar, in fact they are identical to at least 4 decimals accuracy. This is to be expected as they both are based on numerical maximization of the likelihood function.

\begin{table}[!h]
\centering
\begin{tabular}{| l | r | r|}
\hline
Variable & Bootstrap estimate & MLE estimate \\
\hline
$\widehat {\mathrm{\sd}}[\hat \lambda_0]$ & $`r round(result.bootstrap$lambda0.bootstrap.sd,4)`$ & 
`r round(lambda0.sd.mle,4)`\\
$\widehat {\sd}[\hat \lambda_1]$ & $`r round(result.bootstrap$lambda1.bootstrap.sd,4)`$ &
`r round(lambda1.sd.mle,4)`\\
$\widehat {\Corr}[\hat \lambda_0, \hat \lambda_1]$ & $`r round(result.bootstrap$corr.lambda0.lambda1,4)`$ &
`r round(corr.lambda0.lambda1.mle,4)`\\
\hline
\end{tabular}
\caption{Comparison of estimates for standard deviations of and correlation between $\hat \lambda_0$ and $\hat \lambda_1$ computed with bootstraping or direct maximization of likelihood function.}
\label{table:bootstrap_mle_comparison}
\end{table}

In Table \ref{table:bootstrap_mle_comparison} we compare estimates of standard deviations and correlation between the bootstrapping method and through direct maximization of likelihood function. The estimates from the two methods are quite similar, but as expected they differ a little. We conclude that both methods appear to give acceptable results.