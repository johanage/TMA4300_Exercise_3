---
title: "Excercise 3"
subtitle: "TMA4300 Computer intensive statistical methods Spring 2021"
author: 
- "Johan Fredrik Agerup"
- "Arne Rustad"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
   - \usepackage{subfig}
   - \usepackage[ruled,vlined]{algorithm2e}
   - \SetKw{KwBy}{by}
   - \SetKwProg{Init}{init}{}{}
   - \SetKw{length}{length}
   - \SetKw{KwMean}{mean}
   - \SetKw{KwSum}{sum}
   - \SetKw{KwSd}{sd}
   - \SetKw{KwCorr}{corr}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\sd}{\textrm{SD}}
\newcommand{\bias}{\textrm{Bias}}


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(boot)
library(latex2exp)
library(coda)
library(dplyr)
library(tidyverse)
```

# Problem A

## Comparing AR(2) parameter estimators using resampling of residuals

```{r, fig.height=3, fig.width=4}
source("probAdata.R")
plot(data3A$x)
```
We consider an AR(2) model which is specified by the relation:

\begin{equation}\label{AR2}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t
\end{equation}

The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions with respect to $\mathbf{\beta}$:

\begin{align}
Q_{LS} (\mathbf{x}) = \sum_{t=3}^{T} ( x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} )^2
Q_{LA} (\mathbf{x}) = \sum_{t=3}^{T} |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|
\end{align}

Denote the minimisers by $\mathbf{\beta}_{LS}$ and $\mathbf{\beta}_{LA}$ (calculated by ARp.beta.est), and define the estimated residuals to be :
\begin{equation}\label{estimated_residuals}
\hat{e}_t = x_t -\hat{\beta}_1x_{t-1} -\hat{\beta}_2 x_{t-2}
\end{equation}

for $t = 3, . . . , T$, and let $\bar{e}$ be the mean of these. The $\hat{e}_t$ can be re-centered
to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t -\bar{e}$. (Results for $\hat{\epsilon}_t$ obtained by LS and LA can be calculated with ARp.resid).

## A.1

### 1.1 Use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators.

```{r}
source("probAhelp.R", local = knitr::knit_global())
minisers_ar2 = ARp.beta.est(x = data3A$x, p = 2)
eps_residuals_LS = ARp.resid(x = data3A$x, beta = minisers_ar2$LS)
eps_residuals_LA = ARp.resid(x = data3A$x, beta = minisers_ar2$LA)

#initialise values for x1 and x2 
#by picking a random consecutive subsequence from the data
sample_consecutive = function(x){
  #set seed for reproducability
  set.seed(2021)
  x_reshaped = matrix(unlist(t(x)), byrow=T, 50, 2)
  idx = sample(seq(1,50), size = 1, replace = FALSE)
  x_sample_pair = x_reshaped[idx,]
  return(x_sample_pair)
}

x1x2_sample1 = sample_consecutive(x = data3A$x)
print(x1x2_sample1)
```
To do a resampling, initialise values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data.
```{r, fig.height=3, fig.width=4}
calc_x_LS = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LS, 
                       e = sample(eps_residuals_LS, size=length(eps_residuals_LS), replace=TRUE))
plot(calc_x_LS)
```

```{r, fig.height=3, fig.width=4}
calc_x_LA = ARp.filter(x0 = x1x2_sample1, 
                       beta = minisers_ar2$LA, 
                       e = sample(eps_residuals_LA, size=length(eps_residuals_LA), replace=TRUE))
plot(calc_x_LA)
```

### 1.2 Specifically, estimate the variance and bias of the two estimators.

```{r}

"
Agenda:
- implement theta_hat*(b) which is the empirical variance of the bootstrapsample for every t
  - and for b = 1,2,...,B
- Estimate the empirical SD(theta_hat*) by the formula for SE_B^
- compute theta_hat*(dot)

"

# bootstrap resampling
residual_bootstrap_resampling = function(x, p, B){
  samples_LS = c()
  samples_LA = c()
  sd_LS = c()
  sd_LA = c()
  for(i in 1:B){
    " Done equally for both methods LS, LA
    Algo:
    1. Sample pair x1 and x2 from the given data
    2. Use help function ARp.beta.est to calculate the betas
    3. Use help function ARp.resid to calc residuals
    4. Use help function ARp.filter to resample w/ replacement
    5. Add sd for current sample to list of sds
    6. Calc the estimator for the SD
    "
    
    x_pair = sample_consecutive(x = x)
    beta_vecs = ARp.beta.est(x = x, p = p)
    eps_LS = ARp.resid(x = x, beta = beta_vecs$LS)
    eps_LA = ARp.resid(x = x, beta = beta_vecs$LA)
    x_LS = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LS, 
                         e = sample(eps_LS, size=length(eps_LS), replace=TRUE))
    sd_LS = c(sd_LS, sqrt(var(x_LS)))
    # print(sqrt(var(x_LS)))
    samples_LS = c(samples_LS, x_LS)
    
    
    x_LA = ARp.filter(x0 = x_pair, 
                         beta = beta_vecs$LA, 
                         e = sample(eps_LA, size=length(eps_LA), replace=TRUE))
    sd_LA = c(sd_LA, sqrt(var(x_LA)))
    samples_LA = c(samples_LA, x_LA)
  }
  hat_SE_B_LS = sqrt(sum((sd_LS - mean(sd_LS))^2)/(B -1))
  hat_SE_B_LA = sqrt(sum((sd_LA - mean(sd_LA))^2)/(B -1))
  df_boot = data.frame(LS = samples_LS, LA = samples_LA, 
                       est_sd_LS = hat_SE_B_LS,
                       est_sd_LA = hat_SE_B_LA)
  return(df_boot)
}

#estimating  the variance and bias of the two estimators
boot1 = residual_bootstrap_resampling(x = data3A$x, p = 2, B = 2000)
# hist(data3A$x)
# hist(boot1$LS)
# hist(boot1$LA)
# var_boot1_LS = var(boot1$LS)
# print(var_boot1_LS)
# var_boot1_LA = var(boot1$LA)
# print(var_boot1_LA)

#computing bias:
print(paste0("Mean of the data", mean(data3A$x)))
bias_LS = boot1$est_sd_LS[1] - mean(data3A$x)
bias_LA = boot1$est_sd_LA[1] - mean(data3A$x)
print(bias_LS)
print(bias_LA)
#estimating variance and bias of the two estimators after bootstrap
```


The LS estimator is optimal for Gaussian AR(p) processes. 

### Explain if it is also optimal for this problem.



# Problem B Permutation Test

```{r}
bilirubin <- read.table("bilirubin.txt",header=T)
head(bilirubin)


"Use a boxplot to inspect the logarithms of the concentrations for each individual. Be careful to use the same y-axis to make the plots comparable. Use the function lm in R to fit the regression model log Yij = βi + epsilon_ij , with i = 1, 2, 3 and j = 1, . . . , ni (1)
where n1 = 11, n2 = 10 and n3 = 8, and epsilon_ij iid∼ N (0, σ2). Use the F-test to test the hypothesis that β1 = β2 = β3 and save the value of the F-statistic as Fval. Is the hypothesis accepted?"
model_bilirubin <- lm(log(meas)~pers,data=bilirubin)
F_statistic = summary.lm(model_bilirubin)

"Write a function permTest() which generates a permutation of the data between the three individuals, consequently fits the model given in (1) and finally returns the value of the F-statistic for testing β1 = β2 = β3."
permTest = function() {return(0)}


"Perform a permutation test using the function permTest to generate a sample of size 999 for the F-statistic. Compute the p-value for Fval using this sample. What do you observe?
"
```



# Problem C


Let $x_1, \dots, x_n$ and $y_1, \dots, y_n$ be independent random variables where the $x_i$'s have an exponential distribution with $\lambda_0$ and the $y_i$'s have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $x_1, \dots, x_n$ and $y_1, \dots, y_n$ directly, but that we observe

\begin{align*}
z_i &= \max \{x_i, y_i\} \qquad \textrm{for } i = 1, \dots, n \\
u_i &= I(x_i \geq y_i) \qquad \textrm{for } i = 1, \dots, n 
\end{align*}


where $I(A) = 1$ if $A$ is true and $0$ otherwise. $Thus for each $i = 1, \dots, n$ we observe the largest value of $x_i$ and $y_i$ and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i) \qquad \textrm{for } i = 1, \dots, n $ we will in this problem use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.


## a)

Define the following notation:

\begin{align*}
\vect{x} = [x_1, \dots, x_n] \\
\vect y = [y_1, \dots, y_n] \\
\vect z = [z_1, \dots, z_n] \\
\vect u = [u_1, \dots, u_n]
\end{align*}

The likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{align*}
  L(\lambda_0, \lambda_1 | \vect x, \vect y) 
  &= f(\vect x, \vect y | \lambda_0, \lambda_1) \\
  &= f(\vect x | \lambda_0) f(\vect y | \lambda_1) \\
  &= \left [ \prod_{i=1}^n f(x_i | \lambda_0) \right ] \left [ \prod_{i=1}^n f(y_i | \lambda_1) \right ]
  &= \left [ \prod_{i=1}^n \lambda_0 e^{- \lambda_0 x_i} \right ]
  \left [ \prod_{i=1}^n \lambda_1 e^{- \lambda_1 y_i} \right ] \\
  &= \lambda_0^n e^{-\lambda_0 \sum_{i=1}^n x_i} \lambda_1^n e^{-\lambda_1 \sum_{i=1}^n y_i}.
\end{align*}

Correspondingly, the log likelihood function for the complete data $(\vect x, \vect y)$ is

\begin{equation*}
\begin{split}
l(\lambda_0, \lambda_1 | \vect x, \vect y) &= \ln \left( L(\lambda_0, \lambda_1 | \vect x, \vect y)  \right) \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n x_i -\lambda_1 \sum_{i=1}^n y_i
\end{split}
\end{equation*}

Using this we want to find the expectation of the log likelihood function for the complete data condtional on the observed data $(\vect z, \vect u)$ and the estimates for the parameters at iteration $t$, i.e $(\lambda_o^{(t)}, \lambda_1^{(t)})$.

\begin{equation} \label{eq:em_formula_incomplete}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
&= \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
| \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
&= n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) -\lambda_0 \sum_{i=1}^n \E [x_i | z_i, u_i, \lambda_0^{(t)}]  - \lambda_1 \sum_{i=1}^n \E [y_i | z_i, u_i, \lambda_1^{(t)}
\end{split}
\end{equation}

For the case $u_i = 1$ then we have

\begin{align*}
\E [x_i | z_i, u_i = 1, \lambda_0^{(t)}] &= u_i
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 1, \lambda_1^{(t)}
&= \int_0^{z_i} y_i P \left( y_i | y_i \leq z_i, \lambda_1^{(t)}\right) dy_i \\
&=\int_0^{z_i} y_i \frac{P \left( y_i \cap y_i \leq z_i | \lambda_1^{(t)} \right) }
{P \left( y_i \leq z_i | \lambda_1^{(t)} \right) } dy_i \\
&= \int_0^{z_i} y_i \frac{f(y_i | \lambda_1^{(t)})}{ \int_0^{z_i} f(y_i^* | \lambda_1^{(t)}) dy_i^*} dy_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i^* } dy_i^*}
\int_0^{z_i} y_i \lambda_1^{(t)} e^{-\lambda_1^{(t)} y_i} dy_i) \\
&= \frac{1}{1 - e^{- \lambda_1^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_1^{(t)} z_i} - \frac{1}{\lambda_1^{(t)}} e^{- \lambda_1^{(t)} z_i} + \frac{1}{\lambda_1^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_1^{(t)} z_i}}{1 - e^{- \lambda_1^{(t)} z_i}} + \frac{1}{\lambda_1^{(t)}} \\
&= \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
\end{align*}

Correspondlingly, we have for the $u_i = 0$ that

\begin{align*}
\E [x_i | z_i, u_i = 0, \lambda_0^{(t)}
&= \int_0^{z_i} x_i P \left( x_i | x_i \leq z_i, \lambda_0^{(t)}\right) dx_i \\
&=\int_0^{z_i} x_i \frac{P \left( x_i \cap x_i \leq z_i | \lambda_0^{(t)} \right) }
{P \left( x_i \leq z_i | \lambda_0^{(t)} \right) } dx_i \\
&= \int_0^{z_i} x_i \frac{f(x_i | \lambda_0^{(t)})}{ \int_0^{z_i} f(x_i^* | \lambda_0^{(t)}) dx_i^*} dx_i) \\
&= \frac{1}{ \int_0^{z_i} \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i^* } dx_i^*}
\int_0^{z_i} x_i \lambda_0^{(t)} e^{-\lambda_0^{(t)} x_i} dx_i) \\
&= \frac{1}{1 - e^{- \lambda_0^{(t)} z_i}} 
\left( 
-z_i e^{- \lambda_0^{(t)} z_i} - \frac{1}{\lambda_0^{(t)}} e^{- \lambda_0^{(t)} z_i} + \frac{1}{\lambda_0^{(t)}}
\right) \\
&= \frac{-z_i e^{- \lambda_0^{(t)} z_i}}{1 - e^{- \lambda_0^{(t)} z_i}} + \frac{1}{\lambda_0^{(t)}} \\
&= \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{align*}

and

\begin{align*}
\E [y_i | z_i, u_i = 0, \lambda_1^{(t)}] &= u_i \\
\end{align*}

Inserting this result into Equation (\ref{eq:em_formula_incomplete}) we get

\begin{equation} \label{eq:em_formula}
\begin{split}
\E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
=& \E \left[ \ln\left( f(\vect x, \vect y | \lambda_0, \lambda_1) \right) 
\big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}  \right] \\
=& n \left( \ln(\lambda_0) + \ln(\lambda_1) \right) \\
&- \lambda_0 \sum_{i=1}^n 
\left [ u_i z_i 
+ (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1} \right)
\right] \\
&- \lambda_1 \sum_{i=1}^n \left [
(1 - u_i) z_i + 
u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1} \right)
\right ]
\end{split}
\end{equation}

## 2)

Let $Q(\lambda_0, \lambda_1)$ be defined

\begin{equation*}
Q(\lambda_0, \lambda_1)
= Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)})
= \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]
\end{equation*}

We want to use the Expectation-Maximization (EM) algorithm as defined in Algorithm \ref{em_algorithm} to find the maximum likelihood estimates of $(\lambda_0, \lambda_1)$ iteratively. We denote these estimates $(\hat \lambda_0, \hat \lambda_1)$.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Observed data $(\vect z, \vect u)$ and starting values $\lambda_0^{(0)}, \lambda_1^{(0)} $}
 \KwOut {Maximum likelihood estimates for $(\lambda_0, \lambda_1)$.}
 $t \gets 0$\;
 \While{not converged}{
  $Q(\lambda_0, \lambda_1) = \E \left[ l(\lambda_0, \lambda_1 | \vect x, \vect y)  \big | \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right]$ \;
  $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)}) = \argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1) $\;
    $t \gets t + 1$ \;
 }
 \Return{$(\hat \lambda_0, \hat \lambda_1) = (\lambda_0^{(t)}, \lambda_1^{(t)})$}
 \caption{EM algorithm to maximize $l(\lambda_0, \lambda_1 |  \vect z, \vect u)$}
 \label{em_algorithm}
\end{algorithm}

In Task C1 we found an expression for $Q(\lambda_0, \lambda_1)$. To be able to implement Algorithm \ref{em_algorithm} we need to find an expression for $\argmax_{(\lambda_0, \lambda_1)} Q(\lambda_0, \lambda_1)$. To maximize $Q(\lambda_0, \lambda_1)$ with respect to $(\lambda_0, \lambda_1)$ we set the partial derivatives equal to zero and solve for each of the parameters.

\begin{align}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_0} - \sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ] = 0 \nonumber \\
  \lambda_0 = \frac{n}
  {\sum_{i=1}^n \left[ u_i z_i 
  + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp \{ \lambda_0^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda0}
\end{align}

\begin{align}
  \frac{\partial}{\partial \lambda_1} Q(\lambda_1, \lambda_1) = 0 \nonumber \\
  \frac{n}{\lambda_1} - \sum_{i=1}^n \left[ (1 - u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ] = 0
  \nonumber \\
  \lambda_1 = \frac{n}
  {\sum_{i=1}^n \left[ (1-u_i) z_i 
  + u_i \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp \{ \lambda_1^{(t)} z_i \} - 1} \right) \right ]} \label{eq:recursive_lambda1}
\end{align}

To ensure we have found a maximum we also check the second derivatives

\begin{align*}
\frac{\partial^2}{\partial \lambda_0^2} &= \frac{-n}{\lambda_0^2} < 0\\
  \frac{\partial^2}{\partial \lambda_1^2} &= \frac{-n}{\lambda_1^2} < 0
\end{align*}

As the second derivatives are stricly negative for all $\lambda_0$ and $\lambda_1$ we know that we have found the values that maximizes $Q(\lambda_0, \lambda_1)$. From Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) we have a recursive expression for the maximum likelihood estimates of $(\lambda_0, \lambda_1)$.

Below we implement the EM-algorithm for this specific case and find the maximum likelihood estimates when the data is as specified in the files \textbf{z.txt} and \textbf{u.txt} available from the course home page. As stopping criteria we use that the maximum absolute value parameter change from one iteration to the next must be smaller than some chosen $\epsilon$, that is 
$\max \left \{ |\lambda_0^{(t+1)} - \lambda_0^{(t)}|, |\lambda_1^{(t+1)} - \lambda_1^{(t)}| \right \} < \epsilon$.

```{r}
# implementation of the EM algorithm. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
em.algorithm = function(z, u, lambda0, lambda1, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # create vectors for storing lambda0 and lambda1 values for each iteration
  lambda0.vec = c(lambda0)
  lambda1.vec = c(lambda1)
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  t = 1
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0.vec = c(lambda0.vec, 
                       n / sum(u * z + (1 - u) * (1 / lambda0.vec[t] - z / (exp(lambda0.vec[t] * z) - 1) ) )
    )
    lambda1.vec = c(lambda1.vec,
                    n / sum((1 - u) * z + u * (1 / lambda1.vec[t] - z / (exp(lambda1.vec[t] * z) - 1) ) )
    )
    t = t + 1
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0.vec[t] - lambda0.vec[t-1]), abs(lambda1.vec[t] - lambda1.vec[t-1]))
  }
  # get the number of iterations
  n.iter = t-1
  # create a data frame for storing parameter values for each iteration for easy plotting
  df = data.frame(iter = seq(0,n.iter), lambda0 = lambda0.vec, lambda1 = lambda1.vec)
  # same data frame on long format (also for easy plotting)
  df.long = pivot_longer(df, -iter, names_to = "Parameter")
  # return a list with useful values
  return ( list(lambda0 = lambda0.vec[t],lambda1 = lambda1.vec[t],
                lambda0.vec = lambda0.vec, lambda1.vec = lambda1.vec, n.iter = n.iter,
                df = df, df.long = df.long))
}
```


```{r}
# fetch the data
z = read.csv("z.txt")[,1]
u = read.csv("u.txt")[,1]
# find maximum likelihood estimates
result.em = em.algorithm(z, u, 1, 1)
```

The maximum likelihood estimates of $(\lambda_0, \lambda_1)$ are $(\hat \lambda_0, \hat \lambda_1) = (`r round(result.em$lambda0, 3)`, `r round(result.em$lambda1, 3)`)$. Next we test the convergence of the algorithm for different initial values.

```{r, fig.width=8, fig.height = 3, fig.cap = "\\label{fig:convergence_plot} Visualization of convergence for each of the parameters $(\\lambda_0,\\lambda_1)$ for three different initial values $(\\lambda_0^{(0)}, \\lambda_1^{(0)}) \\in [(1,1), (7,7), (15, 15)]$. As stopping criteria we use $\\textrm{max} [ |\\lambda_0^{(t+1)} - \\lambda_0^{(t)}|, |\\lambda_1^{(t+1)} - \\lambda_1^{(t)}| ] < 10^{-6}$."}
# test for different initial values
# choose the initial value combinations
initial.lambda0 = c(1, 7, 15)
initial.lambda1 = c(1, 7, 15)

# create a data frame to store all the realizations
df.em.long = data.frame()
for (i in 1:length(initial.lambda0)) {
  result.em.i = em.algorithm(z, u, initial.lambda0[i], initial.lambda1[i])
  df.em.long = rbind(df.em.long, cbind(result.em.i$df.long, "initial.value" = toString(i)))
}
# visialize convergence for different initial values
ggplot(df.em.long, aes(x = iter, y = value, col = initial.value)) + geom_line() +
  
  facet_wrap(~Parameter) + guides(col = guide_legend("Initial values")) +
  
  scale_color_discrete(labels = c("(1,1)", "(7,7)", "(15,15)")) +
  
  ggtitle("Convergence plot of estimates")
```

From Figure \ref{fig:convergence_plot} we see that the algorithm quickly converges for all three initial conditions. The estimate for $\lambda_0$ appear to be almost converged after only a single iteration, while the estimate for $\lambda_1$ seem to need around 5 iterations for the same level of precicion. After 16 iterations all three initial positions were converged according to $\epsilon = 10^{-6}$.

## 3)

In this task we use bootstrapping to estimate the standard deviations and the biases of each of $\hat \lambda_0$ and $\hat \lambda_1$ along with an estimate for $\Corr [\hat \lambda_0, \hat \lambda_1]$. First we define som notation. We will bootstrap $B$ new datasets $(\vect z^{b*}, \vect u^{b*}), \; b = 1, \dots, B$. These datasets will each be used to calculate the two estimators of interest $(\hat \lambda_0^{b*}, \hat \lambda_1^{b*})$ using Algorithm \ref{em_algorithm} on $(\vect z^{b*}, \vect u^{b*})$. From the $B$ pairs of parameter estimates we can estimate the standard deviations of $\hat \lambda_0$ and $\hat \lambda_1$ in addition to the correlation between them.

\begin{align*}
  \widehat {\sd}_{B}[\hat \lambda_0] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)^2} \\
  \widehat {\sd}_{B}[\hat \lambda_1] &= \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\lambda_1^{b*} -\bar \lambda_1^*)^2} \\
  \widehat {\Corr}_{B}[\hat \lambda_0, \hat \lambda_1] &= \sqrt{\frac{1}{B-1} 
  \sum_{b=1}^B (\lambda_0^{b*} -\bar \lambda_0^*)(\lambda_1^{b*} -\bar \lambda_1^*)}
\end{align*}

where

\begin{align*}
  \bar \lambda_0^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_0^{b*} \\
  \bar \lambda_1^* &= \frac{1}{B} \sum_{b=1}^{B} \lambda_1^{b*}.
\end{align*}

Additionally, we can calculate an estimate for the biases of $\hat \lambda_0$ and $\hat \lambda_1$ using the expressions below.

\begin{equation} \label{eq:bias}
  \begin{split}
  \widehat {\bias}_B [\hat \lambda_0] &= \hat \lambda_0 - \bar \lambda_0^{*} \\
  \widehat {\bias}_B [\hat \lambda_1] &= \hat \lambda_1 - \bar \lambda_1^{*}
  \end{split}
\end{equation}

An improved bias estimate can also be calculated, but requires more complex notation to properly explain. Let $F$ be the cumulative distribution function that we assume $(\vect z, \vect u)$ is sampled from and let $\theta = (\lambda_0, \lambda_1) = T(F)$ be a functional of $F$. We let $\hat F$ be the empirical distribution function of the observed data $(\vect z, \vect u)$ and $\hat F^{b*}, \; b = 1, \dots, B$ be the empirical distribution functon of the corresponding bootstrap sample $(\vect z^{b*}, \vect u^{b*})$. The statistical function of bias can be written as $R((\vect Z, \vect U), F) = T(\hat F) - T(F)$. The bias estimate in Equation (\ref{eq:bias}) was derived as $\frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat F) \right)$. According to \cite{} an improved bias estimate is

\begin{equation*}
 \widehat {\bias}_{B, \textrm{improved}}[\hat \lambda_0, \hat \lambda_1] = \hat {R}((\vect Z, \vect U), F) = \frac{1}{B} \sum_{b=1}^B \left( T(\hat F^{b*}) - T(\hat \bar F^*) \right)
 = \bar \theta^* - T(\bar F^*)
\end{equation*}

??????????????????????? Should I use widehat over whole R expression?

where

\begin{equation*}
  \bar F^* = \frac{1}{B} \sum_{b=1}^B \hat F^{b*}
\end{equation*}

and $\bar \theta^* = (\bar \lambda_0^*, \bar \lambda_1^*)$.

Let $T_{\lambda_0}(F) = \lambda_0$ and $T_{\lambda_1}(F) = \lambda_1$. Then $T(\bar F^*)$ can be calculated through the EM algorithm with each step being

\begin{align*}
\lambda_0 &= \frac{n} {\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ u_i^{b*} z_i^{b*} 
+ (1 - u_i^{b*}) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_0^{(t)} z_i^{b*} \} - 1} \right) \right]} \\
\lambda_1 &= \frac{n}{\frac{1}{B} \sum_{b=1}^B \sum_{i=1}^n \left[ (1-u_i^{b*}) z_i^{b*}
+ u_i^{b*} \left( \frac{1}{\lambda_1^{(t)}} - \frac{z_i^{b*}}{\exp \{ \lambda_1^{(t)} z_i^{b*} \} - 1} \right) \right ]}
\end{align*}

The derivation for this is not included, but is quite similar to how Equation (\ref{eq:recursive_lambda0}) and (\ref{eq:recursive_lambda1}) were derived.

\begin{algorithm}[H]
\SetAlgoLined
 \KwIn {Starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$, observed data $(\vect z, \vect u)$ and the number of bootstrap samples $B$.}
 \KwOut {Estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Calculates two different bias estimates.}
 \Init{}{
 $ \Lambda_B \gets$ Matrix(nrow : $B$, ncol : $2$) \;
 $n \gets \length(\vect z)$ \;
 $\vect z_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 $\vect u_{B, \textrm{all}} \gets$ Vector(size : $B \cdot n$) \;
 }
 \For{$b \gets 1$ \KwTo $B$ \KwBy $1$} {
 $\vect z^{b*} \gets$ bootstrapped version of $\vect z$ of length $n$. \;
 $\vect u^{b*} \gets$ bootstrapped version of $\vect u$ of length $n$. \;
 $(\lambda_0^{b*}, \lambda_1^{b*}) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z^{b*}, \vect u^{b*})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\Lambda_B[b,] \gets (\lambda_0^{b*}, \lambda_1^{b*})$  \;
 $\vect z_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect z^{b*}$ \;
 $\vect u_{B, \textrm{all}}[(1+(b-1) \cdot B) : (b\cdot B)] \gets \vect u^{b*}$ \;
 }
 $\widehat {\sd}_B[\hat \lambda_0] \gets \KwSd(\Lambda[,1])$\;
 $\widehat {\sd}_B[\hat \lambda_1] \gets \KwSd(\Lambda[,2])$\;
 $\widehat {\Corr}[\hat \lambda_0, \hat \lambda_1] \gets \KwCorr(\Lambda[,1], \Lambda[,2])$\;
 $\bar \lambda_0^* \gets \KwMean(\Lambda[,1])$\;
 $\bar \lambda_1^* \gets \KwMean(\Lambda[,2])$\;
 $(\hat \lambda_0, \hat \lambda_1) \gets$  maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z, \vect u)$  as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_B[\hat \lambda_0] \gets \bar \lambda_0^* - \hat \lambda_0$\;
 $\widehat {\bias}_B[\hat \lambda_1] \gets \bar \lambda_1^* - \hat \lambda_1$\;
 $(T_{\lambda_0}(\bar F^*), T_{\lambda_1}(\bar F^*)) \gets$ maximum likelihood estimates calculated using Algorithm \ref{em_algorithm} with $(\vect z_{B, \textrm{all}}, \vect u_{B, \textrm{all}})$ as input observed data and starting values $(\lambda_0^{(0)}, \lambda_1^{(0)})$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0] \gets \bar \lambda_0^* - T_{\lambda_0}(\bar F^*)$\;
 $\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1] \gets \bar \lambda_1^* - T_{\lambda_1}(\bar F^*)$\;
 \SetArgSty{textrm}
 \Return{$\widehat {\mathrm{\sd}}_B[\hat \lambda_0], \widehat {\sd}_B[\hat \lambda_1],  \widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1],\widehat {\bias}_B[\hat \lambda_0], \widehat {\bias}_B[\hat \lambda_1], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0], \widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$}z
 \caption{Bootstrap algorithm used in task C3}
 \label{bootstrap_algorithm}
\end{algorithm}

In Algorithm \ref{bootstrap_algorithm} it is presented pseudocode for a bootstrap algorithm for calculating estimates of the standard deviations and biases of $(\hat \lambda_0, \hat \lambda_1)$ along with correlation between $\hat \lambda_0$ and $\hat \lambda_1$. Below it is implemented in code.


```{r}
# fast implementation of the EM algorithm that only stores and returns
# the final values of lambda0 and lambda1. 
# Function for finding maximum likelihood estimates of lambda0 and lambda1 
fast.em.algorithm = function(z, u, lambdas.initial, epsilon = 1e-6) {
  # initialize delta as higher than stopping critera
  delta = 1 + epsilon
  # Create variable for storing previous iteration value of lambda0 and lambda1 values for each iteration
  lambda0.prev = lambdas.initial[1]
  lambda1.prev = lambdas.initial[2]
  # calculate the length of z for use in the argmax expressions
  n = length(z)
  while (delta > epsilon) {
    # calculate new lambda0 and lambda1 values
    lambda0 = n / sum(u * z + (1 - u) * (1 / lambda0.prev - z / (exp(lambda0.prev * z) - 1) )
    )
    lambda1 = n / sum((1 - u) * z + u * (1 / lambda1.prev - z / (exp(lambda1.prev * z) - 1) )
    )
    
    # update delta to check if stopping criteria is satisfied
    delta = max(abs(lambda0 - lambda0.prev), abs(lambda1 - lambda1.prev))
    
    # update previous iteration value of lamba0 and lambda1 with newly calculated values
    lambda0.prev = lambda0
    lambda1.prev = lambda1
  }
  # return final estimates for lambda0 and lambda1
  return ( c(lambda0, lambda1))
}
```


```{r}
# function for performing what is described in the bootstrap algorithm
bootstrap.lambdas = function(z, u, B = 1000, initial.lambda0 = 1, initial.lambda1 = 1, epsilon = 1e-6) {
  "
  Input:
  - z: observed data as defined earlier (max of x_i and y_i, for i = 1,...,n)
  - u: observed data as defined earlier (1 if x_i >= y_i else 0, for i = 1, ...,n)
  - B: number of bootstrapped samples to use
  - initial.lambda0: initial value for lambda0 for use in em algorithm
  - initial.lambda1 initial value for lambda1 for use in em algorithm
  Output:
  - A list containing
    -- lambda0.est: maximum likelihood estimate for lambda0
    -- lambda1.est: maximum likelihood estimate for lambda1
    -- lambda0.bootstrap.mean: mean of maximum likelihood estimators for lambda0 based
    on bootstrapped data.
    -- lambda1.bootstrap.mean: mean of maximum likelihood estimators for lambda1 based
    on bootstrapped data.
    -- lambda0.bias.est: estimate of bias for lambda0 estimate
    -- lambda1.bias.est: estimate of bias for lambda1 estimate
    -- lambda0.bias.est.improved: improvedestimate of bias for lambda0 estimate
    -- lambda1.bias.est.improved: improvd estimate of bias for lambda1 estimate
    -- corr.lambda0.lambda1: esimate for the correlation between lambda0 and lambda1 estimators 
  "
  
  
  # initialize matrix for store bootstrapped lambda0 and lambda1 estimates
  lambdas.bootstrap = matrix(NA, ncol = 2, nrow = B+1)
  # the first row is only used for initializing the algorithm and will later be removed
  lambdas.bootstrap[1,] = c(initial.lambda0, initial.lambda1)
  n = length(z)
  # initialize vectors for storing all bootstraped data
  z.bootstrap.all = rep(NA, B*n)
  u.bootstrap.all = rep(NA, B*n)
  for (i in 1:B) {
    # draw which observations should be used
    drawn.obs = sample(1:n, n, replace=TRUE)
    # create new bootstrapped datasets
    z.bootstrap = z[drawn.obs]
    u.bootstrap = u[drawn.obs]
    # using the bootstrapped data, calculate the maximum likelihood estimates of
    # lambda0 and lambda1 using em algorithm
    lambdas.bootstrap[i+1,] = fast.em.algorithm(z.bootstrap, u.bootstrap, lambdas.bootstrap[i,], epsilon)
    # store the bootstrapped datasets
    z.bootstrap.all[(1+n*(i-1)):(n*i)] = z.bootstrap
    u.bootstrap.all[(1+n*(i-1)):(n*i)] = u.bootstrap
    
  }
  # removing the initial values from the lambdas matrix
  lambdas.bootstrap = lambdas.bootstrap[-1,]
  
  # calculating the mean of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.mean = apply(lambdas.bootstrap, 2, mean)
   # calculating the standard deviation of the bootstrapped estimators of lambda0 and lambda1
  lambdas.bootstrap.sd = apply(lambdas.bootstrap, 2, sd)
  
  # calculate estimates for lambda0 and lambda1 using original observed data
  lambdas.est = fast.em.algorithm(z, u, lambdas.bootstrap.mean, epsilon)
  
    # calculate T(\bar F^*) for use to calculate improved bias estimates
  lambdas.est.from.mean.cumulative.dist = fast.em.algorithm(z.bootstrap.all, u.bootstrap.all,
                                                            c(1,1), epsilon)
  
  return(list(lambda0.est = lambdas.est[1],
              
              lambda1.est = lambdas.est[2],
              
              lambda0.bootstrap.mean = lambdas.bootstrap.mean[1],
              
              lambda1.bootstrap.mean = lambdas.bootstrap.mean[2],
              
              lambda0.bias.est = lambdas.bootstrap.mean[1] - lambdas.est[1],
              
              lambda1.bias.est = lambdas.bootstrap.mean[2] - lambdas.est[2],
              
              lambda0.bias.est.improved = lambdas.bootstrap.mean[1] - lambdas.est.from.mean.cumulative.dist[1],
              
              lambda1.bias.est.improved = lambdas.bootstrap.mean[2] - lambdas.est.from.mean.cumulative.dist[2],
              
              lambda0.bootstrap.sd = lambdas.bootstrap.sd[1],
              
              lambda1.bootstrap.sd = lambdas.bootstrap.sd[2],
              
              corr.lambda0.lambda1 = corr(lambdas.bootstrap)
              ))
}

set.seed(1)
result.bootstrap = bootstrap.lambdas(z, u, B = 2000, epsilon = 1e-6)
```

In Table \ref{table:bootstrap_output} it is listed the standard deviation, biases and correlation between $(\hat \lambda_0, \hat \lambda_1)$. All the estimates are calculated using Algorithm \ref{bootstrap_algorithm}. Both of the standard deviation estimates and the correlation estimate appear reasonable. We observe that there is hardly any correlation between the two estimators $\hat \lambda_0$ and $\hat \lambda_1$.

\begin{table}[!h]
\centering
\begin{tabular}{| l | r |}
\hline
Variable & Bootstrap estimate \\
\hline
$\widehat {\mathrm{\sd}}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bootstrap.sd,3)` \\
$\widehat {\sd}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bootstrap.sd,3)` \\
$\widehat {\Corr}_B[\hat \lambda_0, \hat \lambda_1]$ & `r round(result.bootstrap$corr.lambda0.lambda1,3)` \\
$\widehat {\bias}_B[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est,3)`\\ 
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_0]$ & `r round(result.bootstrap$lambda0.bias.est.improved,3)`\\
$\widehat {\bias}_B[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est,3)`\\
$\widehat {\bias}_{B,\textrm{improved}}[\hat \lambda_1]$ & `r round(result.bootstrap$lambda1.bias.est.improved,3)`\\
\hline
\end{tabular}
\caption{Table containing the output from Algorithm \ref{bootstrap_algorithm} with $(\vect z, \vect u)$ as input observed data, $(\lambda_0^{(0)}, \lambda_1^{(0)}) = (1,1)$ and $B = 2000$.}
\label{table:bootstrap_output}
\end{table}

From Table \ref{table:bootstrap_output} we see that the bias estimates for $\hat \lambda_0$ is very similar for both methods for estimating bias. For $\hat \lambda_1$, however, the improved bias estimate is around twice as large as the other bias estimate. We choose to rely on the improved bias estimate.


